{"version":"1","records":[{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL"},"type":"lvl1","url":"/hello-triangle","position":0},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL"},"content":"For our first WebGL example we are going to display a simple triangle to the screen, this “hello triangle” is the computer graphics version of the classic “hello world!” example.","type":"content","url":"/hello-triangle","position":1},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Setup HTML and JavaScript Files"},"type":"lvl2","url":"/hello-triangle#setup-html-and-javascript-files","position":2},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Setup HTML and JavaScript Files"},"content":"To start with we need two files, an HTML file that includes a <canvas> element which acts as a rendering surface for WebGL, and JavaScript file that controls everything that happens inside the canvas using the WebGL API.\n\nThese notes use a code-along principle in that as you work through these notes you will be tasked to create files, enter text and run code (the first of these tasks is below). Through working this way you will build an understanding of the concepts and techniques used in computer graphics. Where you are asked to enter code into your files, try to avoid the temptation of simply copying and pasting code, instead type out the code. Whilst this does have the risk of making typos and creating bugs, it does have an effect of helping you understand what each line of code is doing.\n\nTask\n\nCreate a folder called 01 Hello Triangle inside which create an HTML file called index.html and enter the following code.<!doctype html>\n\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n    <title>Lab 1 - Hello Triangle</title>\n\n    <style>\n      html, head, body {\n        margin: 0;\n        padding: 25px;\n        background-color: #000;\n      }\n\n      #demo-canvas {\n        width: 800px;\n        height: 600px;\n        background-color: #8AcE00; \n        image-rendering: crisp-edges;\n      }\n    </style>\n  </head>\n  <body>\n    <canvas id=\"demo-canvas\" width=\"800px\" height=\"600px\">\n      HTML5 canvas not supported in your browser!\n    </canvas>\n\n    <script src=\"hello_triangle.js\"></script>\n  </body>\n</html>\n\nIf you open your index.html file in a web browser (e.g., Chrome) you will see that we have created a <canvas> element that is 800 pixels wide by 600 pixels high that is a particularly lurid shade of green. The reason for this horrible colour is that we will be clearing the canvas in our WebGL app so if we see this green colour we know something has gone wrong.\n\n\n\nFigure 1:The lurid green canvas element.\n\nTask\n\nInside your 01 Hello Triangle folder, create a JavaScript file hello_triangle.js that contains the following.// Main function\nfunction main() {\n\n  // Setup WebGL\n  const canvas = document.getElementById('demo-canvas');\n  const gl = canvas.getContext('webgl2');\n  if (!gl) return alert(\"WebGL not supported\");\n\n  // Clear the canvas\n  canvas.width = canvas.clientWidth;\n  canvas.height = canvas.clientHeight;\n  gl.viewport(0, 0, canvas.width, canvas.height);\n  gl.clearColor(0.2, 0.2, 0.2, 1.0); // dark gray background\n  gl.clear(gl.COLOR_BUFFER_BIT);\n}\n\ntry {\n  main();\n} catch (e) {\n  console.error(`Uncaught JavaScript exception: ${e}`);\n}\n\nHere we have created our main function main() inside which we have set up the WebGL canvas. Some functions used here are defined below\n\ngl.viewport(0, 0, canvas.width, canvas.height);\n\nDefines the rectangular area of the canvas where rendering will take place that maps to the normalized device co-ordinates (-1 to 1 in the x, y and z axes). Here our viewport fills the <canvas> element.\n\ngl.clearColor(0.2, 0.2, 0.2, 1.0);\n\nDefines the background colour. Colours are defined using RGBA values (Red, Green, Blue and Alpha) so here our background is dark grey.\n\ngl.clear(gl.COLOR_BUFFER_BIT);\n\nClears the specified buffer, in this case it’s the colour buffer.\n\nRefresh the browser window and you should still see that the horrible lurid green background. But hang on, haven’t we defined our background colour to be dark grey? The reason for this is that we haven’t embedded the JavaScript file into our HTML file.\n\nTask\n\nEdit the <body> tag near the bottom of the index.html file so that it looks like the following....\n<body>\n  <canvas id=\"demo-canvas\" width=\"800px\" height=\"600px\">\n    HTML5 canvas not supported in your browser! These demos will not work.\n  </canvas>\n\n  <script src=\"hello_triangle.js\"></script>\n</body>\n...\n\nNow if you refresh your browser you should see a dark grey background. If something has gone wrong, and you still see the green background, open up the JavaScript console (if you are using Chrome then press CTRL + SHIFT + J or ⌥ + ⌘ + J on a Mac) and it should give you an indication of what has gone wrong. The life of a graphics programmer is mostly problem-solving and debugging, so get used to doing this.\n\n","type":"content","url":"/hello-triangle#setup-html-and-javascript-files","position":3},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Draw a Red Triangle"},"type":"lvl2","url":"/hello-triangle#draw-a-red-triangle","position":4},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Draw a Red Triangle"},"content":"The first shape we are going to draw with WebGL is a red triangle. We start with a triangle because it is the fundamental building block of modern computer graphics. Every complex 3D model, character, environment, object etc. are ultimately made from many small triangles, and WebGL (like all graphics hardware) is designed to process triangles extremely efficiently.\n\nTriangles are used in computer graphics for several important reasons:\n\nA triangle is always flat.\nAny three points in space define a single plane, which means a triangle cannot become bent or warped. This makes it reliable for representing surfaces.\n\nThey are the simplest possible polygon.\nWith only three vertices, triangles are easy for the GPU to transform, rasterise, and shade. All calculations, such as clipping and interpolation, are simpler with triangles than with more complex shapes.\n\nGraphics hardware is optimized for triangles.\nModern GPUs expect triangles as input, and even if you provide quads or other polygons, the hardware will convert them into triangles internally.\n\nAny shape can be built from triangles.\nComplex models and smooth surfaces can be approximated by dividing them into many small triangles, a process known as tessellation. This allows triangles to represent everything from simple shapes to highly detailed 3D geometry.\n\nInterpolation works cleanly across triangles.\nColours, textures, normals, and depth all interpolate smoothly inside a triangle using barycentric co-ordinates, which ensures correct shading and rendering.","type":"content","url":"/hello-triangle#draw-a-red-triangle","position":5},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Define the Triangle Co-ordinates","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#triangle-coordinates-section","position":6},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Define the Triangle Co-ordinates","lvl2":"Draw a Red Triangle"},"content":"Our triangle will have co-ordinates at (-0.5, -0.5, 0) (lower-left vertex), (0.5, -0.5, 0) (lower-right vertex) and (0, 0.5, 0) (top vertex).\n\n\n\nFigure 3:The vertex co-ordinates for the red triangle example.\n\nTask\n\nEnter the following just before we clear the canvas.// Define triangle vertices\nconst triangleVertices = new Float32Array([\n  // x     y    z\n   -0.5, -0.5, 0.0, // vertex 0       2\n    0.5, -0.5, 0.0, // vertex 1     /   \\\n    0.0,  0.5, 0.0, // vertex 2    0 --- 1\n]);\n\nHere we have defined an array called triangleVertices containing 9 values for the co-ordinates of the 3 triangle vertices. WebGL works best with 32-bit floats, and the default JavaScript float precision is 64-bit, so we use the new Float32Array() command to convert these into 32-bit float array.","type":"content","url":"/hello-triangle#triangle-coordinates-section","position":7},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Create a Vertex Buffer Object (VBO) for the Triangle","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#lab01-triangle-buffer","position":8},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Create a Vertex Buffer Object (VBO) for the Triangle","lvl2":"Draw a Red Triangle"},"content":"The data in the triangleVertices array is stored in the CPU (RAM) and not in the GPU. To move data across to the GPU we create a WebGL buffer object known as a Vertex Buffer Object (VBO) and copy in the data.\n\nTask\n\nEnter the following after we have defined the triangle vertices array.// Create a VBO for the triangle\nconst VBO = gl.createBuffer();\ngl.bindBuffer(gl.ARRAY_BUFFER, VBO);\ngl.bufferData(gl.ARRAY_BUFFER, triangleVertices, gl.STATIC_DRAW);\ngl.bindBuffer(gl.ARRAY_BUFFER, null);\n\nThe commands used here are explained below\n\nconst VBO = gl.createBuffer();\n\nCreates a buffer object on the GPU.\n\ngl.bindBuffer(gl.ARRAY_BUFFER, VBO);\n\nBinds our VBO to an array buffer so that WebGL knows where to send the data. The word bind in graphics programming means to make it the currently active resource for a particular purpose so subsequent WebGL operations affect it.\n\ngl.bufferData(gl.ARRAY_BUFFER, triangleVertices, gl.STATIC_DRAW);\n\nCopies the data from the triangleVertices array into the VBO which is the currently bound buffer. The gl.STATIC_DRAW input is a performance hint to WebGL, here we are saying that that triangle vertices will not change.\n\ngl.bindBuffer(gl.ARRAY_BUFFER, null);\n\nHere we unbind the current array buffer, so no subsequent commands will mistakenly affect it.","type":"content","url":"/hello-triangle#lab01-triangle-buffer","position":9},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Write and Compile the Vertex Shader","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#lab01-vertex-shader","position":10},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Write and Compile the Vertex Shader","lvl2":"Draw a Red Triangle"},"content":"The next step is create a shader program that runs on the GPU and uses WebGL to determine which pixels on the display are to be rendered and in what colour. This shader program comprises two separate programs known as “shaders”: the vertex shader and the fragment shader. Shaders are written in GLSL (webGL Shader Language) which is similar to C.\n\nThe vertex shader is called once for each vertex and transforms the vertex co-ordinates from model space (the local object co-ordinates) to the clip space that defines the region that is displayed on the canvas. We have defined our triangle using co-ordinates between -1 and 1, so they are already in clip space and our vertex shader just needs to output each vertex. The shader code for a simple vertex shader is#version 300 es\nprecision mediump float;\n\nin vec3 aPosition;\n\nvoid main() {\n  gl_Position = vec4(aPosition, 1.0);\n}\n\nThis code is explained below\n\n#version 300 es\n\nTells the shader we are using GLSL ES 3.00, the shader language used with WebGL 2.0.\n\nprecision mediump float;\n\nSets the default float precision to medium (16-bit).\n\nin vec3 aPosition;\n\nTells the shader that we are inputting a 3-element vector for the vertex position. The a in aPosition is short for attribute.\n\ngl_Position = vec4(aPosition, 1.0);\n\nOutputs a 4-element vector for the clip space co-ordinates of the vertex (there reason why it’s a 4-element vector will be covered later when we look at transformations). The gl_Position variable is a required output of every vertex shader.\n\nThe simplest way of entering shader code into our JavaScript file is to define it as a multiline string.\n\nTask\n\nAdd the following at the top of the hello_triangle.js file. Note the use of backticks `...` to define the string using a literal so that we can use a multiline string.// Define vertex shader\nconst vertexShaderSource = \n`#version 300 es\nprecision mediump float;\n\nin vec3 aPosition;\n\nvoid main() {\n  gl_Position = vec4(aPosition, 1.0);\n}`;\n\nNow add the following after we have created the VBO.// Compile vertex shader\nconst vertexShader = gl.createShader(gl.VERTEX_SHADER);\ngl.shaderSource(vertexShader, vertexShaderSource);\ngl.compileShader(vertexShader);\nif (!gl.getShaderParameter(vertexShader, gl.COMPILE_STATUS)) {\n  console.log(`Error compiling vertex shader:\\n`, gl.getShaderInfoLog(vertexShader));\n  gl.deleteShader(vertexShader);\n}\n\nAs well as defining a string for the vertex shader code, we then create a vertex shader object, attach the shader code to it and compile it. There’s no easy way to check for errors in shader code, so it is good practice to do a check to see if it has compiled ok, if not a message is logged to the console.","type":"content","url":"/hello-triangle#lab01-vertex-shader","position":11},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Write and Compile the Fragment Shader","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#lab01-fragment-shader","position":12},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Write and Compile the Fragment Shader","lvl2":"Draw a Red Triangle"},"content":"The other shader we need to write is the fragment shader. This takes in information on a fragment that has been identified as being part of the primitive (i.e., triangle) and determines the colour that it will be rendered. We are going to render all fragments in the triangle as red, so the fragment shader code is as follows.#version 300 es\nprecision mediump float;\n\nout vec4 outColour;\n\nvoid main() {\n  outColour = vec4(1.0, 0.0, 0.0, 1.0);\n}\n\nThis is similar to the vertex shader with a couple of exceptions. The vertex shader has a required output of gl_Position, so this doesn’t need to be declared, fragment shaders can have a number of outputs, so we need to declare these. Here we have declared an output of a 4-element vector using out vec4 outColour;. Within the main() function, we set the output vector to (1, 0, 0, 1), i.e., all red with no blue or green components and the Alpha value is set to 1 so that it is opaque.\n\nTask\n\nAdd the following after we have defined the vertex shader.// Define fragment shader\nconst fragmentShaderSource = \n`#version 300 es\nprecision mediump float;\n\nout vec4 outColour;\n\nvoid main() {\n  outColour = vec4(1.0, 0.0, 0.0, 1.0);\n}`;\n\nAnd add the following after we have compiled the vertex shader.// Compile fragment shader\nconst fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);\ngl.shaderSource(fragmentShader, fragmentShaderSource);\ngl.compileShader(fragmentShader);\nif (!gl.getShaderParameter(fragmentShader, gl.COMPILE_STATUS)) {\n  console.log(`Error compiling fragment shader:\\n`, gl.getShaderInfoLog(fragmentShader));\n  gl.deleteShader(fragmentShader);\n}\n\nThis code is very similar to the one used for the vertex shader. Later we will be using a helper function for DRY (Don’t Repeat Yourself).","type":"content","url":"/hello-triangle#lab01-fragment-shader","position":13},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Link the Shaders into a WebGL Program","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#lab01-webgl-program","position":14},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Link the Shaders into a WebGL Program","lvl2":"Draw a Red Triangle"},"content":"The next step is to create a WebGL program and link the vertex and fragment shader to it. In doing this it manages the inputs and outputs, attributes and uniforms between our two shaders.\n\nTask\n\nAdd the following after we have compiled the vertex and fragment shaders.// Create WebGL shader program and link the vertex and fragment shaders\nconst shaderProgram = gl.createProgram();\ngl.attachShader(shaderProgram, vertexShader);\ngl.attachShader(shaderProgram, fragmentShader);\ngl.linkProgram(shaderProgram);\nif (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {\n console.log(`Failed to link WebGL program : ${gl.getProgramInfoLog(shaderProgram)}`);\n return;\n}\n\nThe commands used here are similar to those used for the vertex and fragment shader. Instead of compiling the program we are simply linking the two shaders using the gl.linkProgram() function. We also do a check to see if the linking has been successful.","type":"content","url":"/hello-triangle#lab01-webgl-program","position":15},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Draw the Triangle","lvl2":"Draw a Red Triangle"},"type":"lvl3","url":"/hello-triangle#lab01-draw-triangle","position":16},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl3":"Draw the Triangle","lvl2":"Draw a Red Triangle"},"content":"So far we have done a fair bit of work creating a buffer object for the triangle vertices, written and compiled the vertex and fragment shader and linked these into a WebGL program. But we haven’t drawn anything yet! To do this we need to tell WebGL which shader program to use, tell it how to interpret the vertex data and then tell it to draw the triangle.\n\nTask\n\nEnter the following after we have cleared the canvas.// Set the shader program\ngl.useProgram(shaderProgram);\n\nThe gl.useProgram() function tells WebGL which shader program to use. It will use the currently selected program until told differently by another call to this function.\n\nEarlier we created the VBO in the GPU and copied across our vertex data. We now need to tell WebGL where this data is and how to read it, so it can work its magic with the shaders.\n\nTask\n\nEnter the following after you have set the shader program.// Tell WebGL how to read data from the vertex buffer  \nconst positionLocation = gl.getAttribLocation(shaderProgram, \"aPosition\");\ngl.enableVertexAttribArray(0);\ngl.bindBuffer(gl.ARRAY_BUFFER, VBO);\ngl.vertexAttribPointer(\n  positionLocation, // index\n  3,                // size\n  gl.FLOAT,         // type\n  false,            // normalized\n  0,                // stride\n  0);               // offset\n\nThe functions used here are explained below;\n\nconst positionLocation = gl.getAttribLocation(shaderProgram, 'aPosition');\n\nGets the location of the aPosition attribute from the WebGL shader program.\n\ngl.enableVertexAttribArray(0);\n\nEnables a vertex attribute with location 0 so that WebGL knows to read data from the VBO and pass it to the vertex shader.\n\ngl.bindBuffer(gl.Array_BUFFER, VBO);\n\nMake the VBO the currently active buffer.\n\ngl.vertexAttribPointer(positionLocation, 3, gl.FLOAT, false, 0, 0);\n\nTells WebGL how to read the data from the currently bound buffer and pass it to the vertex shader attribution 0. The inputs are explained in the table below.\n\nInput\n\nDescription\n\nindex\n\nAttribute location. We stored this in the positionLocation variable using the gl.getAttribLocation() function.\n\nsize\n\nNumber of components per vertex, We have 3D (x, y, z) co-ordinates, so this is 3.\n\ntype\n\nData type. We are using floats.\n\nnormalized\n\nWhether integer values should be mapped to [0, 1] or [-1, 1]. Our vertex co-ordinates are in NDC, so we don’t need to do this.\n\nstride\n\nNumber of bytes between first attribute of each vertex. Our data is tightly packed, so we can set the stride to 0, if the array buffer contained other data, e.g., colours, then we would need to determine the number of bytes between the first attribute, e.g., the x co-ordinates.\n\noffset\n\nNumber of bytes from the start of the buffer the attribute of the first vertex. Our first vertex co-ordinate is the first element in the buffer, so this is 0.\n\nThe last thing we need to do is actually tell WebGL to draw the triangle.\n\nTask\n\nEnter the following after the you have told WebGL how to read the data from the vertex buffer.// Draw the triangle\ngl.drawArrays(gl.TRIANGLES, 0, 3);\n\nThe gl.drawArrays() function tells the GPU to draw the primitives. The inputs are the type of primitive (we have a triangle), the index of the first vertex and the number of vertices to draw (we have one triangle, so we want to draw 3 vertices). Refresh your browser window, pray to the programming gods and if everything has gone to plan you should see the red triangle displayed on the canvas.\n\n\n\nFigure 4:Hello triangle!","type":"content","url":"/hello-triangle#lab01-draw-triangle","position":17},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Steps Used to Draw the Triangle"},"type":"lvl2","url":"/hello-triangle#steps-used-to-draw-the-triangle","position":18},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Steps Used to Draw the Triangle"},"content":"Now that we have successfully drawn a triangle, let’s recap the steps we used.\n\nDefine the Triangle Co-ordinates - we defined a 9-element array that contained the (x, y, z) co-ordinates of the 3 triangle vertices.\n\nCreate a Vertex Buffer Object (VBO) for the Triangle - the buffer objects are stored in the GPU so we needed to create one for the triangle and copy across the triangle co-ordinates so WebGL can work with them.\n\nWrite and Compile the Vertex Shader - a vertex shader is used to transform the vertex co-ordinates to the clip space. The output is sent to the WebGL rasteriser that passes the fragment co-ordinates to the fragment shader.\n\nWrite and Compile the Fragment Shader - a fragment shader is used to determine the colour that the fragment should be rendered on the display.\n\nLink the Shaders into a WebGL Program - the vertex and fragment shaders are linked into a single program that we tell WebGL to use to draw the triangle.\n\nDraw the Triangle - we need to tell WebGL how to access the data in the triangle buffer and instruct it to draw the triangle.","type":"content","url":"/hello-triangle#steps-used-to-draw-the-triangle","position":19},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Shader Compiler Utility Functions"},"type":"lvl2","url":"/hello-triangle#shader-compiler-utility-functions","position":20},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Shader Compiler Utility Functions"},"content":"Looking at our main() function we see that the code to compile and link the shaders is very similar for the vertex and fragment shaders. In the spirit of DRY, we will now write some utility functions to simplify the code.\n\nTask\n\nCreate a new JavaScript files called webGLUtils.js within the 01 Hello Triangle/ folder and enter the following code.// Compile shader helper function\nfunction compileShader(gl, type, code) {\n  const shader = gl.createShader(type);\n  gl.shaderSource(shader, code);\n  gl.compileShader(shader);\n  \n  if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {\n    console.log(`Error compiling ${shaderType === gl.VERTEX_SHADER ? 'vertex' : 'fragment'} shader:\\n`, gl.getShaderInfoLog(shader));\n    gl.deleteShader(shader);\n    return;\n  }\n\n  return shader;\n}\n\n// Link shaders into a shader program\nfunction createShaderProgram(gl, vertexShaderSource, fragmentShaderSource) {\n  const vertexShader = compileShader(gl, gl.VERTEX_SHADER, vertexShaderSource);\n  const fragmentShader = compileShader(gl, gl.FRAGMENT_SHADER, fragmentShaderSource);\n\n  if (!vertexShader || !fragmentShader) {\n    return;\n  }\n\n  const shaderProgram = gl.createProgram();\n  gl.attachShader(shaderProgram, vertexShader);\n  gl.attachShader(shaderProgram, fragmentShader);\n  gl.linkProgram(shaderProgram);\n\n  if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {\n    console.log('Error linking shader program:\\n', gl.getProgramInfoLog(shaderProgram));\n    gl.deleteProgram(program);\n    return;\n  }\n\n  return shaderProgram;\n}\n\nThis new file webGLUtils.js is used to contain any utility (helper) functions that are required for WebGL. Here we have defined the functions compileShader() which compiles a vertex or fragment shader depending on the type input, and createShaderProgram() that calls compileShader() to create the vertex and fragment shaders and links them to create a WebGL program. If you compare them to the equivalent code in the main() function you can see that they are similar. To enable our main file to use these functions we also need to add utils.js to the index.html file using a script tag.\n\nTask\n\nAdd the following just before the hello_triangle.js script tag. <script src=\"webGLUtils.js\"></script>\n\nWe can now tidy up the main() function and make a call to out new createShaderProgram() function.\n\nTask\n\nCut and paste the vertexShaderSource and fragmentShaderSource to the top of the hello_triangle.js file. Delete the code in the main() function that compiles and links the shaders and replace it with the following.// Create WebGL program \nconst shaderProgram = createShaderProgram(gl, vertexShaderSource, fragmentShaderSource);\n\nRefresh your browser window and if everything has gone to plan you should see your red triangle as before. This effort is not wasted, we can now easily create WebGL programs from the source code without lots of copying and pasting of existing code. We will be using the webGLUtils.js and other JavaScript files to help organise and simplify our code in the future.","type":"content","url":"/hello-triangle#shader-compiler-utility-functions","position":21},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Exercises"},"type":"lvl2","url":"/hello-triangle#exercises","position":22},{"hierarchy":{"lvl1":"Lab 1: Drawing a Simple Shape in WebGL","lvl2":"Exercises"},"content":"Change the vertex shader so that the following results are achieved.\n\n  (a) The triangle is shifted by 0.75 to the right.\n\n\n\nHint\n\nWe can access individual elements of the aPosition vectors using aPosition.x, aPosition.y and aPosition.z -- this is known as \n\nswizzling.\n\n  (b) The triangle is drawn upside-down.\n\n\n\n  (c) The x and y co-ordinates are swapped.\n\n\n\nUse two triangles to draw a green rectangle with lower-left vertex at (-0.5, -0.5, 0) and the upper-right vertex at (0.5, 0.5, 0).\n\n\n\nUse triangles to draw a blue hexagon.\n\n\n\nHint\n\nWe can draw a hexagon using 6 triangles where each triangle has one vertex at (0,0,0) and two outer vertices that lie on the circumference of a circle.\n\n\n\nThe x and y vertices of the two outer vertices are\\begin{align*}\n  x_i &= r \\cos(\\theta_i), & y_i &= r \\sin(\\theta_i), \\\\\n  x_{i+1} &= r \\cos(\\theta_{i+1}), & y_{i+1} &= r \\sin(\\theta_{i+1}),\n\\end{align*}\n\nwhere r is the radius and the angles \\theta_i and \\theta_{i+1} are calculated using\\begin{align*}\n  \\theta_i &= \\frac{i2\\pi}{6}, \\\\\n  \\theta_{i+1} &= \\frac{(i+1)2\\pi}{6}.\n\\end{align*}\n\nUse lots of triangles to draw a yellow circle.\n\n","type":"content","url":"/hello-triangle#exercises","position":23},{"hierarchy":{"lvl1":"Lab 2: Colours"},"type":"lvl1","url":"/colours","position":0},{"hierarchy":{"lvl1":"Lab 2: Colours"},"content":"In the previous page we were able to draw triangles using WebGL of a single colour. Whilst this is awesome, wouldn’t it be better if we were able to draw triangles using different colours. In this page we will see how we can add colour data to each vertex, add more shapes to the scene and make use of VAOs (Vertex Array Objects) and EBOs (Element Buffer Objects).","type":"content","url":"/colours","position":1},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Hello Colourful Triangle"},"type":"lvl2","url":"/colours#hello-colourful-triangle","position":2},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Hello Colourful Triangle"},"content":"We are going to build upon the code we used to draw the boring red triangle example and jazz it up a bit by defined colour data for each vertex. Instead of starting from scratch, let’s copy the code from the previous page.\n\nTask\n\nCreate a folder called 02 Colours and download \n\nindex.html, \n\ncolours.js and \n\nwebGLUtils.js to it. Open index.html in a web browser to check that the red triangle from \n\nLab 1 is displayed.\n\n\n\nRecall that the basic steps to draw the hello triangle were\n\nWrite and compile a vertex and fragment shader and link these in a WebGL program.\n\nCreate a vertex buffer on the GPU and copy across vertex co-ordinate data from the CPU.\n\nTell WebGL how to read the vertex buffer.\n\nTell WebGL to draw the triangle.\n\nTo add colour data to our triangle we need to modify the vertex and fragment shaders to work with an additional input of colour data, add a buffer containing the colours of the vertices and tell WebGL how to read this buffer.","type":"content","url":"/colours#hello-colourful-triangle","position":3},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl3":"Vertex and Fragment Shaders","lvl2":"Hello Colourful Triangle"},"type":"lvl3","url":"/colours#vertex-and-fragment-shaders","position":4},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl3":"Vertex and Fragment Shaders","lvl2":"Hello Colourful Triangle"},"content":"Our current vertex shader simply takes in an input of a 3-element vector containing the (x, y, z) vertex co-ordinates and outputs a 4-element vector of these co-ordinates to the fragment shader. The vertex co-ordinates are outputted using the gl_Position variable which is a required output and is not specifically declared. To add colour data to our co-ordinates we need to declare a second 3-element input vector for the attribute colour as well as an output vector for the vertex colour. So our vertex shader is#version 300 es\nprecision mediump float;\n\nin vec3 aPosition;\nin vec3 aColour;\n\nout vec3 vColour;\n\nvoid main() {\n  gl_Position = vec4(aPosition, 1.0);\n\n  // Output fragment colour\n  vColour = aColour;\n}\n\nAs for the fragment shader, we will simply output the colour data of the vertex. Our current fragment shader does not have an input declared because it expects gl_Position by default, so we need to add an input declaration for the vertex colour outputted by the vertex shader and use this to create the 4-element RGBA output vector.#version 300 es\nprecision mediump float;\n\nin vec3 vColour;\n\nout vec4 outColour;\n\nvoid main() {\n  outColour = vec4(vColour, 1.0);\n}\n\nTask\n\nEdit the vertex shader code at the top of the colours.js file so that is contains the modified vertex and fragment shaders shown above.","type":"content","url":"/colours#vertex-and-fragment-shaders","position":5},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl3":"Vertex Colours","lvl2":"Hello Colourful Triangle"},"type":"lvl3","url":"/colours#vertex-colours","position":6},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl3":"Vertex Colours","lvl2":"Hello Colourful Triangle"},"content":"To add colour data to the triangle vertices we add 3 more float values for the red, green and blue colours to the triangleVertices array.\n\nTask\n\nAmend the triangleVertices array so that it looks like the following.// Define triangle vertices\nconst triangleVertices = new Float32Array([\n  // x     y    z        r    g    b\n   -0.5, -0.5, 0.0,     1.0, 0.0, 0.0, // vertex 0       2\n    0.5, -0.5, 0.0,     0.0, 1.0, 0.0, // vertex 1     /   \\\n    0.0,  0.5, 0.0,     0.0, 0.0, 1.0, // vertex 2    0 --- 1\n]);\n\nHere we have specified vertex 0 is pure red, vertex 1 is pure green and vertex 2 is pure blue. If you refresh your web browser you should see that the red triangle is now black, and it appears that the top vertex has been moved to the right-hand edge of the canvas (). The reason why its skewed is that when we told WebGL how to read the vertex buffer using gl.vertexAttribPointer(colourLocation, 3, gl.FLOAT, false, 0, 0);. This means that WebGL is expected no gaps between the vertex co-ordinate data since the stride input (the second to last input) is 0, so it thinks the 3 vertex co-ordinates are (-0.5, -0.5, 0), (1, 0, 0) and (0.5, -0.5, 0).\n\n\n\nFigure 2:Oops, something has gone wrong.\n\nThis is where the stride input for the gl.vertexAttribPointer() function comes in. Stride is the number of bytes from the start of the attribute (in our case the x vertex co-ordinate) for one vertex to the start of the same attribute of the next vertex. We added 3 floats for the RGB data, so our stride is the number of bytes used to store 6 float values, i.e., 3 for the (x, y, z) values and 3 for the RGB values.\n\n\n\nFigure 3:The stride and offset of a vertex array.\n\nTask\n\nChange the gl.vertexAttribPointer(); function so that it looks like the following.gl.vertexAttribPointer(\n  positionLocation,                   // index\n  3,                                  // size\n  gl.FLOAT,                           // type\n  false,                              // normalized\n  6 * Float32Array.BYTES_PER_ELEMENT, // stride\n  0);                                 // offset\n\nRefresh your browser, and you should see that the triangle vertices has returned to its previous state, but it’s still black (\n\nFigure 4). The reason for this is that we have not yet told WebGL about our new vertex colours. To do this we get the location of the aColour attribute from the vertex shader, enable the attribute array and point WebGL to where it can find the colour data.\n\n\n\nFigure 4:Sorted the vertex co-ordinates but not the colour.\n\nTask\n\nEnter the following code after we told WebGL how to read the co-ordinate data.const colourLocation = gl.getAttribLocation(shaderProgram, 'aColour');\ngl.enableVertexAttribArray(colourLocation);\ngl.vertexAttribPointer(\n  colourLocation,                      // index\n  3,                                   // size\n  gl.FLOAT,                            // type\n  false,                               // normalized\n  6 * Float32Array.BYTES_PER_ELEMENT,  // stride\n  3 * Float32Array.BYTES_PER_ELEMENT); // offset\n\nNote that here the offset value is 3 lots of the number of bytes used to store a 32-bit float. This is because the colour data comes after the 3 floats for the co-ordinate values.\n\nRefresh your browser you should see the triangle in all its glorious colourfulness.\n\n\n\nFigure 5:Hello colourful triangle!\n\nYou can see that the 3 triangle vertices are red, green and blue going anti-clockwise from the bottom-left vertex. The colour of the pixels across the interior of the triangle have been interpolated by the rasteriser so that we have a smooth transition of colours.","type":"content","url":"/colours#vertex-colours","position":7},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"More Shapes"},"type":"lvl2","url":"/colours#more-shapes","position":8},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"More Shapes"},"content":"To keep our colourful triangle company we are going to introduce another shape, a square, to our scene. In most graphics applications shapes are constructed using triangle primitives. Triangles are the simplest 2D shape and all other shapes are constructed using triangles. So to construct a square we need two triangles.\n\nTask\n\nEnter the following code after we defined the triangle vertex buffer.// Define square vertices\nconst squareVertices = new Float32Array([\n  // x    y    z       r    g    b             \n    0.5, 0.2, 0.0,    1.0, 0.0, 0.0, // vertex 0     3 -- 2\n    0.8, 0.2, 0.0,    0.0, 1.0, 0.0, // vertex 1     |  / |    \n    0.8, 0.6, 0.0,    0.0, 0.0, 1.0, // vertex 2     | /  | \n    0.5, 0.2, 0.0,    1.0, 0.0, 0.0, // vertex 0     0 -- 1 \n    0.8, 0.6, 0.0,    0.0, 0.0, 1.0, // vertex 2\n    0.5, 0.6, 0.0,    1.0, 1.0, 1.0, // vertex 3\n]);\n\n// Create VBO for the square\nconst squareVBO = gl.createBuffer();\ngl.bindBuffer(gl.ARRAY_BUFFER, squareVBO);\ngl.bufferData(gl.ARRAY_BUFFER, squareVertices, gl.STATIC_DRAW);\ngl.bindBuffer(gl.ARRAY_BUFFER, null);\n\nHere we have defined the co-ordinates and colours for vertices of a square. We are using 2 triangles to construct the square, so we have 6 vertices in total. Like with the triangle, we have also created a vertex buffer for the square and copied the data across to the GPU. We now need to tell WebGL how to read the square vertex buffer and to draw the 2 triangles that make up the square.\n\nTask\n\nEnter the following code after we have drawn the triangle.// Tell WebGL how to read data from the square vertex buffer\ngl.bindBuffer(gl.ARRAY_BUFFER, squareVBO);\ngl.enableVertexAttribArray(positionLocation);\ngl.vertexAttribPointer(positionLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 0);\ngl.enableVertexAttribArray(colourLocation);\ngl.vertexAttribPointer(colourLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 3 * Float32Array.BYTES_PER_ELEMENT);\n\n// Draw the square\ngl.drawArrays(gl.TRIANGLES, 0, 6);\n\nThis is very similar to what we did for the triangle object, i.e., we bind the vertex buffer for the square, enable the vertex attribute array and tell WebGL where the data is. Note that we already know the location of the position and colour attributes. Since we are drawing two squares, we change the 3 to a 6 in the gl.drawArrays() function.\n\n","type":"content","url":"/colours#more-shapes","position":9},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Vertex Array Objects (VAO)"},"type":"lvl2","url":"/colours#vertex-array-objects-vao","position":10},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Vertex Array Objects (VAO)"},"content":"You may have noticed that telling WebGL how to read the VBOs for both the triangle and square objects required repetition of code. Every time we draw a different object we are repeating this code, with just 2 objects this isn’t too bad, but when we are dealing with hundreds of objects it will become unmanageable. A Vertex Array Object (VAO) is a WebGL object that stores all the state related to the vertex, i.e., vertex attributes and bindings to attribute locations, so that once it is set up we only need a single block of code to bind the VAO and draw the object.\n\nTask\n\nDelete (or comment out) the code used to tell WebGL how to interpret the vertex buffers for the triangle and square as well as the draw command.\n\nEnter the following code after we have created the shader program.// Get attribute locations\nconst positionLocation = gl.getAttribLocation(shaderProgram, \"aPosition\");\nconst colourLocation = gl.getAttribLocation(shaderProgram, \"aColour\");\n\n// Create VAO for the triangle\nconst triangleVAO = gl.createVertexArray();\ngl.bindVertexArray(triangleVAO);\ngl.bindBuffer(gl.ARRAY_BUFFER, VBO);\ngl.enableVertexAttribArray(positionLocation);\ngl.vertexAttribPointer(positionLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 0);\ngl.enableVertexAttribArray(colourLocation);\ngl.vertexAttribPointer(colourLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 3 * Float32Array.BYTES_PER_ELEMENT);\ngl.bindVertexArray(null);\n\n// Create VAO for the triangle\nconst squareVAO = gl.createVertexArray();\ngl.bindVertexArray(squareVAO);\ngl.bindBuffer(gl.ARRAY_BUFFER, squareVBO);\ngl.enableVertexAttribArray(positionLocation);\ngl.vertexAttribPointer(positionLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 0);\ngl.enableVertexAttribArray(colourLocation);\ngl.vertexAttribPointer(colourLocation, 3, gl.FLOAT, false, 6 * Float32Array.BYTES_PER_ELEMENT, 3 * Float32Array.BYTES_PER_ELEMENT);\ngl.bindVertexArray(null);\n\nEach VAO is created using the gl.createVertexArray() function. We then bind the vertex array, copy across the vertex data to the GPU and tell WebGL how to access this data. The last thing we do for each VAO is to unbind it using gl.bindVertexArray(null) so that we don’t accidentally make changes to it.\n\nNow whenever we want to draw the triangle or square we just need to bind it’s VAO and called the draw command.\n\nTask\n\nEnter the following code after we have created the VAOs.// Draw triangle\ngl.bindVertexArray(triangleVAO);\ngl.drawArrays(gl.TRIANGLES, 0, 3);\n\n// Draw square\ngl.bindVertexArray(squareVAO);\ngl.drawArrays(gl.TRIANGLES, 0, 6);\n\nRefresh your web browser, and you should see that we still have the colourful triangle and square, good news as it means the VAOs are working.","type":"content","url":"/colours#vertex-array-objects-vao","position":11},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Element Buffer Objects"},"type":"lvl2","url":"/colours#element-buffer-objects","position":12},{"hierarchy":{"lvl1":"Lab 2: Colours","lvl2":"Element Buffer Objects"},"content":"To add the square to our scene we used 6 vertices, 3 for each of the triangle. Two of these vertices, 0 and 2, are shared by both triangles, so we are using more memory than required. To avoid redundant vertex data being created and using up memory, we can define an array of the indices for each triangle which maps to the vertex data. The buffer that we use to store the indices is called the Element Buffer Object (EBO).\n\nTask\n\nReplace the code used to define the square vertex co-ordinates so that it looks like the following.// Define square vertices\nconst squareVertices = new Float32Array([\n // x    y    z       r    g    b             \n   0.5, 0.2, 0.0,    1.0, 0.0, 0.0, // vertex 0     3 -- 2\n   0.8, 0.2, 0.0,    0.0, 1.0, 0.0, // vertex 1     |  / |    \n   0.8, 0.6, 0.0,    0.0, 0.0, 1.0, // vertex 2     | /  | \n   0.5, 0.6, 0.0,    1.0, 1.0, 1.0, // vertex 3     0 -- 1 \n]);\n\n// Define square indices\nconst squareIndices = new Uint16Array([\n0, 1, 2,  // lower-right triangle\n0, 2, 3,  // upper-left triangle\n]);\n\nHere we have defined the co-ordinates and colours for the 4 vertices of the square, as well as an additional array of integers for the indices. We have converted the indices array from 64-bit to 16-bit unsigned integers (non-negative) for use with WebGL using new Uint16Array(). Since we have another array we need to create a buffer for it and copy the data across to the GPU.\n\nTask\n\nEnter the following code after the vertex buffer is created for the square.// Create square EBO\nconst squareEBO = gl.createBuffer();\ngl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, squareEBO);\ngl.bufferData(gl.ELEMENT_ARRAY_BUFFER, squareIndices, gl.STATIC_DRAW);\ngl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, null);\n\nThese commands a similar those used for the vertex buffer. Note that here we need to specify that we have an element array buffer instead of a standard array buffer. The index buffer does not contain any data specific to the vertices, so we don’t need to add it to a VAO, instead we bind it whilst the VAO is bound and WebGL ‘remembers’ it as part of the VAO’s state.\n\nTask\n\nEnter the following before the square VAO is unbound.gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, squareIndexBuffer);\n\nFinally, we need to tell WebGL that our data for the square is defined using indices, so we need to change the draw command.\n\nTask\n\nReplace the gl.drawArrays() command for the square with the following.gl.drawElements(gl.TRIANGLES, 6, gl.UNSIGNED_SHORT, 0);\n\nRefresh your browser, and you should see that the output has not changed, but we can now draw different objects easily by binding its VAO and using the draw command.","type":"content","url":"/colours#element-buffer-objects","position":13},{"hierarchy":{"lvl1":"Lab 3: Textures"},"type":"lvl1","url":"/textures","position":0},{"hierarchy":{"lvl1":"Lab 3: Textures"},"content":"Texture mapping is a technique for applying a 2D image known as a texture onto a 3D surface. Applying a texture adds detail and complexity to the appearance of 3D objects without the need for modelling intricate geometry.\n\n\n\nFigure 1:Mapping a texture to a polygon.\n\nThe texture is a 2D image where each pixel within the texture, known as a textel, is referenced using the texture coordinates given as (u,v) where u and v are in the range 0 to 1, i.e., (0,0) corresponds to the textel in the bottom-left corner and (1,1) corresponds to the textel in the top-right corner. When a fragment is created by the shader the corresponding texture coordinates are calculated and the sample colour of the textel is used for the fragment. Fortunately we do not need to write a texture mapper functions since these are in WebGL.\n\nTask\n\nCreate a folder on your computer titled 03 Textures/ and download the files \n\nindex.html, \n\ntextures.js and \n\nutils.js to it. Load index.html, and you should see a rectangle drawn in the centre of the canvas with different colours applied to the four vertices.\n\n","type":"content","url":"/textures","position":1},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Creating a texture"},"type":"lvl2","url":"/textures#creating-a-texture","position":2},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Creating a texture"},"content":"The first thing we are going to do is create a 4 \\times 4 pixel texture with alternating black and white pixels.\n\nTask\n\nEnter the following after the EBO has been created.// create 4x4 texture\nconst pixels = new Uint8Array([\n  255, 255, 255,    0, 0, 0,          255, 255, 255,    0, 0, 0,\n  0, 0, 0,          255, 255, 255,    0, 0, 0,          255, 255, 255,\n  255, 255, 255,    0, 0, 0,          255, 255, 255,    0, 0, 0,\n  0, 0, 0,          255, 255, 255,    0, 0, 0,          255, 255, 255,\n]);\n\nNow that we have defined an array for the pixels we now need to create a WebGL texture object.\n\nTask// Create texture\nconst texture = gl.createTexture();\ngl.bindTexture(gl.TEXTURE_2D, texture);\ngl.texImage2D(gl.TEXTURE_2D, 0, gl.RGB, 4, 4, 0, gl.RGB, gl.UNSIGNED_BYTE, pixels);\ngl.generateMipmap(gl.TEXTURE_2D);\ngl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);\ngl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);\n\ngl.createTexture();\n\nCreates a WebGL texture object.\n\ngl.texImage2D(gl.TEXTURE_2D, 0, gl.RGB, 4, 4, 0, gl.RGB, gl.UNSIGNED_BYTE, pixels);\n\nSpecifies a 2D texture image. We have a 4 \\times 4 texture that we have defined using RGB hence we are using these inputs (see \n\nKhronos reference pages for details of the inputs).\n\ngl.generateMipmap()\n\nGenerates the mipmaps for this texture (see \n\nbelow).\n\ngl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST)\n\nSets a texture parameter. Here we have set the minificiation and magnification parameters for our 2D texture so that it uses the nearest pixel in the texture to the screen pixel (see \n\ntexture filtering)","type":"content","url":"/textures#creating-a-texture","position":3},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture coordinates"},"type":"lvl2","url":"/textures#texture-coordinates","position":4},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture coordinates"},"content":"So we have created a texture and told WebGL all about it. Now we need to specify how we want to use the texture. To do this for each of the vertices of the triangle we need to define the corresponding (u, v) texture coordinates where (0,0) is the bottom left-hand corner and (1,1) is the top-right hand corner of the texture (see \n\nFigure 1). We already have an array that contains the vertex coordinates and colours, so we can simply add (u, v) coordinates to this.\n\nTask\n\nEdit the vertices array so that the (u,v) coordinates are defined for each vertex.// Define vertices\nconst vertices = new Float32Array([\n  // x    y    z       r    g    b       u    v     \n  -0.5, -0.5, 0.0,    1.0, 0.0, 0.0,    0.0, 0.0, // vertex 0     3 -- 2\n   0.5, -0.5, 0.0,    0.0, 1.0, 0.0,    1.0, 0.0, // vertex 1     |  / |    \n   0.5,  0.5, 0.0,    0.0, 0.0, 1.0,    1.0, 1.0, // vertex 2     | /  | \n  -0.5,  0.5, 0.0,    0.0, 0.0, 0.0,    0.0, 1.0, // vertex 3     0 -- 1 \n]);\n\nHere we have defined the texture coordinates for each vertex so that each vertex of the rectangle is mapped to the corresponding vertex of the texture (we don’t need to do this but since this is our first example it makes sense to keep things simple).\n\nNow that we have added texture coordinates to the vertex buffer we neeed to tell WebGL about them. Each vertex now has 8 datapoints (x, y, z, R, G, B, u and v) so the \n\nstride between the x coordinates of successive vertices is now 8 multiplied by the number of bytes used to store a 32-bit float.\n\nTask\n\nAdd the following to get the location of the texture coordinate attribute where we do the same for positionLocation and colourLocation.const texCoordLocation = gl.getAttribLocation(shaderProgram, \"aTexCoord\");\n\nChange the gl.vertexAttribPointer() function for the vertex coordinates and colour of the vertices so that the stride is changed from 5 to 8.gl.enableVertexAttribArray(positionLocation);\ngl.vertexAttribPointer(positionLocation, 3, gl.FLOAT, false, 8 * Float32Array.BYTES_PER_ELEMENT, 0);\ngl.enableVertexAttribArray(colourLocation);\ngl.vertexAttribPointer(colourLocation, 3, gl.FLOAT, false, 8 * Float32Array.BYTES_PER_ELEMENT, 3 * Float32Array.BYTES_PER_ELEMENT);    \n\nAdd a gl.vertexAttribPointer() function for the texture coordinates.gl.enableVertexAttribArray(texCoordLocation);\ngl.vertexAttribPointer(texCoordLocation, 2, gl.FLOAT, false, 8 * Float32Array.BYTES_PER_ELEMENT, 6 * Float32Array.BYTES_PER_ELEMENT);","type":"content","url":"/textures#texture-coordinates","position":5},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Shaders"},"type":"lvl2","url":"/textures#shaders","position":6},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Shaders"},"content":"The last thing we need to do to apply the texture is to update the vertex and fragment shader. The texture coordinates are read in by the vertex shader and then passed to the rasteriser which calculates the texture coordinates of each fragment.\n\nTask\n\nEdit the vertex shader source code at the top of the textures.js file so that it looks like the following#version 300 es\nprecision mediump float;\n\nin vec3 aPosition; \nin vec3 aColour;\nin vec2 aTexCoord;\n\nout vec3 vColour;\nout vec2 vTexCoord;\n\nvoid main() {\n  /// Output vertex coordinates\n  gl_Position = vec4(aPosition, 1.0);\n\n  // Output vertex colour\n  vColour = aColour;\n\n  // Output texture co-ordinates\n  vTexCoord = aTexCoord;\n}\n\nHere we declare a 2-element input vector aTexCoord and a 2-element output vector vTexCoord for the texture coordinates of the vertex (recall that standard convention is that the prefix a stands for attribute and v stands for vertex). The texture coordinates are outputted without us doing anything to them.\n\nThe fragment shader needs to read in the texture coordinates of the fragment and get the colour of the fragment from the texture.\n\nTask\n\nEdit the fragment shader source code so that it looks like the following#version 300 es\nprecision mediump float;\n\nin vec3 vColour;\nin vec2 vTexCoord;\n\nout vec4 outColour;\n\nuniform sampler2D uSampler;\n\nvoid main() {\n  outColour = texture(uSampler, vTexCoord);\n}\n\nHere we now have an input of the 2-element vector vTexCoord which has been outputted from the vertex shader. Since our texture is a 2D image then we use the sampler2D GLSL type to declare the uniform textureMap (\n\nuniforms are explained below). The colour of the fragment is taken from the texture using the texture() function where the first argument is the name of the texture uniform and the second argument is the texture coordinates of the fragment.\n\nRefresh your web browser, and you should see that a checkerboard texture has been applied to the rectangle.\n\n\n\nHere we have defined a target called texture which is an integer used to refer to the texture. The texture is then generated and bound to this target using the glGenTextures() and glBindTexture() functions. -->\n\nWe now need to load an image into our texture. To do this we are going to make use of the \n\nstb_image library, the header file for which can be found in the common/ folder. Enter the following code into your program.// Load texture image from file\nconst char *path = \"../assets/crate.jpg\";\nint width, height, nChannels;\nstbi_set_flip_vertically_on_load(true);\nunsigned char *data = stbi_load(path, &width, &height, &nChannels, 0);\n\nif (data)\n    std::cout << \"Texture loaded.\" << std::endl;\nelse\n    std::cout << \"Texture not loaded. Check the path.\" << std::endl;\n\nThe functions used here are:\n\nstbi_set_flip_vertically_on_load() flips the image vertically since the (0,0) coordinate on an image is the top-left corner and WebGL expects it to be the bottom-right corner\n\nstbi_load() loads the image specified in the path string into the data variable and the stores the width, height and number of colour channels into the appropriate variables\n\nThe texture we are using here is crate.jpg which is stored in the assets/ folder and represents a side of a wooden crate.\n\n\n\nThe crate texture.\n\nAfter getting the texture data from the image file we tell WebGL we that have a texture. Enter the following code into your program.// Specify 2D texture\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);\nglGenerateMipmap(GL_TEXTURE_2D);\n\nThe glTexImage2D() function tells WebGL we have a 2D texture which is contained in data. GL_RGB is the format that specifies the number of colours in the texture. Here we are using a JPEG file so the colour each textel is represented by the RGB model. The glGenerateMipmap generates mipmaps for the texture (mipmaps are explained \n\nbelow).\n\nNow that the texture data has been copied to the GPU we can free up memory by adding the following to the program.// Free the image from the memory\nstbi_image_free(data);","type":"content","url":"/textures#shaders","position":7},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture coordinates","lvl2":"Shaders"},"type":"lvl3","url":"/textures#texture-coordinates-1","position":8},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture coordinates","lvl2":"Shaders"},"content":"So we have created a texture and told WebGL all about it. Now we need to specify how we want to use the texture. To do this for each of the vertices of the triangle we need to define the corresponding (u, v) texture coordinates. This is done in the same way as the triangle vertices, i.e., define an array containing the coordinates, create a buffer and copy the coordinates to this buffer.\n\nEnter the following code after we defined the vertices array (you may need to scroll up a bit).// Define texture coordinates\nconst float uv[] = {\n    // u   v\n    0.0f, 0.0f,\n    1.0f, 0.0f,\n    0.5f, 1.0f\n};\n\nHere we have defined an array of float values called uv and specified the texture coordinates so that the bottom-left triangle vertex is mapped to the texture coordinate (0, 0) in the bottom-left corner of the texture, the bottom-right vertex is mapped to (1,0) in the bottom-right corner of the texture and the top vertex is mapped to (0.5, 1) in the middle of the top edge of the texture.\n\nThe buffer for the texture coordinates is created in the same was as for the triangle vertices. Enter the following code after the VBO was created.// Create texture buffer\nunsigned int uvBuffer;\nglGenBuffers(1, &uvBuffer);\nglBindBuffer(GL_ARRAY_BUFFER, uvBuffer);\nglBufferData(GL_ARRAY_BUFFER, sizeof(uv), uv, GL_STATIC_DRAW);\n\nAfter we have created the texture (just before the render loop), add the following code to bind it to the VAO.// Bind the texture to the VAO\nglBindTexture(GL_TEXTURE_2D, texture);\nglBindVertexArray(VAO);\n\nSending the texture coordinates to the GPU is done in the same way as for the VBO. Enter the following code after we send the VBO to the GPU (this is in the render loop so scroll down a bit).// Send the UV buffer to the shaders\nglEnableVertexAttribArray(1);\nglBindBuffer(GL_ARRAY_BUFFER, uvBuffer);\nglVertexAttribPointer(1, 2, GL_FLOAT, GL_FALSE, 0, (void*)0);\n\nNote that since each texture coordinates requires just 2 floats for (u,v) instead of 3 for the vertex coordinates (x,y,z) the second argument in the glVertexAttribPointer() function is 2 instead of 3.","type":"content","url":"/textures#texture-coordinates-1","position":9},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Shaders","lvl2":"Shaders"},"type":"lvl3","url":"/textures#shaders-1","position":10},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Shaders","lvl2":"Shaders"},"content":"","type":"content","url":"/textures#shaders-1","position":11},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Vertex shader","lvl3":"Shaders","lvl2":"Shaders"},"type":"lvl4","url":"/textures#vertex-shader","position":12},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Vertex shader","lvl3":"Shaders","lvl2":"Shaders"},"content":"Recall that the \n\nvertex shader deals with the vertex coordinates and is used by WebGL to calculate the coordinates of the fragment. So in addition to passing the (x, y, z) coordinates of the vertices we must also pass the (u, v) coordinates of the textels that correspond to the triangle vertices.\n\nEdit the vertexShader.glsl file in the Lab03_Textures project so that it looks like the following.#version 330 core\n\n// Inputs\nlayout(location = 0) in vec3 position;\nlayout(location = 1) in vec2 uv;\n\n// Outputs\nout vec2 UV;\n\nvoid main()\n{\n    // Output vertex position\n    gl_Position = vec4(position, 1.0);\n    \n    // Output texture coordinates\n    UV = uv;\n}\n\n\nYou may notice some changes from our vertex shader from \n\n2. Basic Shapes in WebGL. We now have a second input uv which is a 2-element vector which are the (u,v) texture coordinates which are outputted as the 2-element vector UV (remember that the gl_Position vector is passed to the fragment shader by default).","type":"content","url":"/textures#vertex-shader","position":13},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Fragment shader","lvl3":"Shaders","lvl2":"Shaders"},"type":"lvl4","url":"/textures#fragment-shader","position":14},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Fragment shader","lvl3":"Shaders","lvl2":"Shaders"},"content":"The fragment shader is where we need to retrieve the sample colour from the texture. Edit the fragmentShader.glsl file so that it looks like the following.#version 330 core\n\n// Input\nin vec2 UV;\n\n// Output\nout vec3 colour;\n\n// Uniforms\nuniform sampler2D textureMap;\n\nvoid main()\n{\n    colour = vec3(texture(textureMap, UV));\n}\n\n\nHere we now have an input of the 2-element vector uv which has been outputted from the vertex shader. Since our texture is a 2D image then we use the \n\nsampler2D GLSL type to declare the uniform textureMap (uniforms are explained \n\nbelow). The colour of the fragment is taken from the texture using the texture() function where the first argument is the name of the texture uniform and the second argument is the (u,v) texture coordinates of the fragment.\n\nCompile and run program and, after sending a prayer to the programming gods, you should be presented with your triangle which has now been textured using the crate texture.\n\n","type":"content","url":"/textures#fragment-shader","position":15},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture rectangle"},"type":"lvl2","url":"/textures#texture-rectangle","position":16},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture rectangle"},"content":"Our texture triangle is great and all but doesn’t really look like a realistic object. Since the original texture is rectangular, let’s create a rectangle out of two triangles with the appropriate texture mapping.\n\n\n\nA rectangle constructed using two triangles.\n\nSo the lower-right (blue) triangle has vertex coordinates (-0.5, -0.5, 0), (0.5, -0.5, 0) and (0.5, 0.5, 0) and the upper-left (red) triangle has vertex coordinates (-0.5, -0.5, 0), (0.5, 0.5, 0) and (-0.5, 0.5, 0). Change the vertices and uv arrays to the following.// Define vertex positions\nstatic const float vertices[] = {\n    // x     y     z\n    -0.5f, -0.5f, 0.0f, \n     0.5f, -0.5f, 0.0f,\n     0.5f,  0.5f, 0.0f, \n    -0.5f, -0.5f, 0.0f,\n     0.5f,  0.5f, 0.0f,\n    -0.5f,  0.5f, 0.0f\n};\n\n// Define texture coordinates\nstatic const float uv[] = {\n    // u    v\n    0.0f,  0.0f,    // triangle 1\n    1.0f,  0.0f,\n    1.0f,  1.0f,\n    0.0f,  0.0f,    // triangle 2\n    1.0f,  1.0f,\n    0.0f,  1.0f\n};\n\nCompile and run the program, and you should be presented with the more realistic image in \n\nFigure 7.\n\n\n\nFigure 7:Texture mapped onto two triangles that form a rectangle.","type":"content","url":"/textures#texture-rectangle","position":17},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Element Buffer Objects (EBO)","lvl2":"Texture rectangle"},"type":"lvl3","url":"/textures#element-buffer-objects-ebo","position":18},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Element Buffer Objects (EBO)","lvl2":"Texture rectangle"},"content":"Our rectangle is defined using 6 sets of (x,y,z) coordinates for the 2 triangles, but a rectangle only has 4 vertices. This means we are using 2 extra coordinates than we really need to as both of the triangles share the vertices at (-0.5, -0.5, 0) and (0.5, 0.5, 0). This isn’t too bad for our simple rectangle example but for more sophisticated scenes that use thousands of triangles it can be inefficient.\n\nTo improve on this we can use an Element Buffer Object (EBO) that contains indices that map a vertex of the rectangle to the vertices array. Consider \n\nFigure 8 that shows a rectangle drawn using two triangles. The lower-right triangle is formed using the vertices with indices 0, 1 and 2 and the upper-left triangle is formed using vertices with indices 0, 2 and 3.\n\n\n\nFigure 8:The mapping of indices to the rectangle vertices.\n\nComment out the code used to define the vertices and uv arrays and enter the following code.// Define vertex positions\nstatic const float vertices[] = {\n    // x     y     z      index\n    -0.5f, -0.5f, 0.0f,  // 0       3 -- 2\n     0.5f, -0.5f, 0.0f,  // 1       |  / |  \n     0.5f,  0.5f, 0.0f,  // 2       | /  |\n    -0.5f,  0.5f, 0.0f   // 3       0 -- 1\n};\n\n// Define texture coordinates\nstatic const float uv[] = {\n    // u    v      index\n    0.0f,  0.0f,  // 0\n    1.0f,  0.0f,  // 1\n    1.0f,  1.0f,  // 2\n    0.0f,  1.0f,  // 3\n};\n\n// Define indices\nstatic const unsigned int indices[] = {\n    0, 1, 2,  // lower-right triangle\n    0, 2, 3   // upper-left triangle\n};\n\nAs with the other buffer objects we need to create a buffer for the indices, bind it and copy the data across. Enter the following code after we’ve created the texture buffer.// Create Element Buffer Object (EBO)\nunsigned int EBO;\nglGenBuffers(1, &EBO);\nglBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);\nglBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);\n\nThis is similar to the code used to create the other buffer objects with the exception we are creating a GL_ELEMENT_ARRAY_BUFFER instead of a GL_ARRAY_BUFFER. The last change we need to make in order to use our EBO is to change the function used to draw the triangles from glDrawArrays() to glDrawElements()// Draw the triangles\nglDrawElements(GL_TRIANGLES, sizeof(indices) / sizeof(unsigned int), GL_UNSIGNED_INT, 0);\n\nglDisableVertexAttribArray(0);\nglDisableVertexAttribArray(1);\n\nMake this change, compile and run the program, and you should see the window from \n\nFigure 7. You may be thinking you’ve gone to all of that trouble only for the rectangle to look exactly the same. Well, now we are using fewer floats in the vertices array, and we can now use EBOs to draw more sophisticated shapes and 3D models.","type":"content","url":"/textures#element-buffer-objects-ebo","position":19},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture wrapping and filtering"},"type":"lvl2","url":"/textures#texture-wrapping-and-filtering","position":20},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Texture wrapping and filtering"},"content":"","type":"content","url":"/textures#texture-wrapping-and-filtering","position":21},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture wrapping","lvl2":"Texture wrapping and filtering"},"type":"lvl3","url":"/textures#texture-wrapping","position":22},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture wrapping","lvl2":"Texture wrapping and filtering"},"content":"In our examples above, all the texture coordinates have been in the range from 0 to 1. What happens if we use textures coordinates outside this range? To test this we are going to change our texture to something less symmetrical (you will see why in a minute). Change the path variable to the following// Load texture image from file\nconst char *path = \"../assets/mario.png\";\n\nPNG (Portable Network Graphics) files use the RGBA colour model which is the standard RGB model with an addition Alpha value that determines the opacity of the colours. We need to let WebGL know that our texture is defined using RGBA so change instances GL_RGB in the glTexImage2D() function to GL_RGBA, i.e.,// Specify 2D texture\nglTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, width, height, 0, GL_RGBA, GL_UNSIGNED_BYTE, data);\n\nCompile and run the program, and you should be presented with a (hopefully) familiar face.\n\n\n\nFigure 9:Its a me, Mario!\n\nNow we can experiment with specifying texture coordinates outside the range 0 to 1. Edit the uv array to change all the 1.0f values to 2.0f. Compile and run the program, and you should see the image shown in \n\nFigure 10\n\n\n\nFigure 10:Texture wrapping using GL_REPEAT.\n\nHere WebGL has used texture wrapping to repeat the texture over the rectangle. This can be useful if we want to use a small texture containing a pattern over a larger polygon, e.g., think of brick wall where the pattern repeats itself.\n\nWebGL offers other options for texture wrapping;\n\nGL_REPEAT - the texture repeats over the fragment (default);\n\nGL_MIRRORED_REPEAT - same as GL_REPEAT but the texture is mirrored with each repeat;\n\nGL_CLAMP_TO_EDGE - clamps the texture coordinates to between 0 and 1, coordinates outside this range are clamped to the edge so that the textels on the edge are stretched to the edge of the fragment;\n\nGL_CLAMP_TO_BORDER - coordinates outside the range (0,0) to (1,1) are given a used defined border colour.\n\nWe can specify the texture wrapping using the glTexParameteri() function. To apply GL_MIRRORED_REPEAT add the following code after the texture as been specified.// Set texture wrapping options\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_MIRRORED_REPEAT);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_MIRRORED_REPEAT);\n\nHere we have specified the wrapping in the horizontal (S) and vertical (T) directions (some people use (s,t) for the vertex coordinates instead of (u,v)) to GL_MIRRORED_REPEAT. Compile and run the program, and you should see the image shown in \n\nFigure 11.\n\n\n\nFigure 11:Texture wrapping using GL_MIRRORED_REPEAT.\n\nUsing GL_CLAMP_TO_EDGE and GL_CLAMP_TO_BORDER instead results in the images shown in \n\nFigure 12 and \n\nFigure 12.\n\n\n\nFigure 12:Texture wrapping using GL_CLAMP_TO_EDGE.\n\n\n\nFigure 13:Texture wrapping using GL_CLAMP_TO_BORDER.","type":"content","url":"/textures#texture-wrapping","position":23},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"type":"lvl3","url":"/textures#texture-filtering-section","position":24},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"content":"Texture filtering is method of determining the colour of the fragment, known as the colour sample, from the texture. WebGL maps the coordinates of the fragment to the texture coordinates and in most cases this will not align exactly to a textel centre, so what does WebGL do? WebGL provides two main options : nearest-neighbour interpolation and bilinear interpolation.","type":"content","url":"/textures#texture-filtering-section","position":25},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Nearest neighbour interpolation","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"type":"lvl4","url":"/textures#nearest-neighbour-interpolation","position":26},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Nearest neighbour interpolation","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"content":"Nearest neighbour interpolation is the default in WebGL uses the colour of the nearest textel to the texture coordinates as the colour sample. This is illustrated in the diagram shown in \n\nFigure 14 where the texture coordinates represented by the black circle is mapped in a region on the texture with four neighbouring textels with the textel centres represented by the crosses. The texture coordinates are closest to the centre of the textel in the top-left so the colour of that textel is used for the colour sample.\n\n\n\nFigure 14:Nearest neighbour interpolation.\n\nTo apply texture filtering we specify the type of interpolation we want in using glTexParameteri() functions. To apply nearest neighbour interpolation add the following code to your program.glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);\n\nThe GL_TEXTURE_MIN_FILTER and GL_TEXTURE_MAG_FILTER arguments refer to minification and magnification. Minification is where the texture is larger than the polygon it is being mapped to so a fragment covers multiple textels. Magnification is the opposite where the texture is smaller than the polygon so that a single textel takes up multiple fragments. We can set different interpolation for magnification and minification.\n\nTo demonstrate the affects of minification lets use a low resolution texture. Change the path variable to the following// Load texture image from file\nconst char *path = \"../assets/mario_small.png\";\n\nAnd change the (u, v) coordinates back so that the texture fills the rectangle. Compile and run the program, and you should see the image shown in \n\nFigure 15.\n\n\n\nFigure 15:Nearest neighbour interpolation\n\nAs you can see the texture mapped rectangle has a block aliased look since multiple fragments share the same textel.","type":"content","url":"/textures#nearest-neighbour-interpolation","position":27},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Bilinear interpolation","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"type":"lvl4","url":"/textures#bilinear-interpolation","position":28},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl4":"Bilinear interpolation","lvl3":"Texture filtering","lvl2":"Texture wrapping and filtering"},"content":"Another method is to calculate the sample colour using \n\nbilinear interpolation where the distance between (u,v) coordinate and the centre of a textel determines how much that textel contributes to the sample colour, i.e., the closer the textel the more of the textel colour is contained in the colour sample.\n\n\n\nFigure 16:Bilinear interpolation.\n\nTo see the affects of bilinear interpolation, change GL_NEAREST to GL_LINEAR in the glTexParameteri() functions. Compile and run the program, and you should see the image shown in \n\nFigure 17.\n\n\n\nFigure 17:Nearest neighbour interpolation\n\nHere we have an improved texture mapping where the aliasing is less noticeable.","type":"content","url":"/textures#bilinear-interpolation","position":29},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Mipmaps"},"type":"lvl2","url":"/textures#mipmaps-section","position":30},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Mipmaps"},"content":"Another issue that may occur is when the fragment is a lot smaller than the texture which can happen when an object that is far away from the viewer. In these cases WebGL will struggle to get the colour sample from a high resolution texture since a single fragment covers a large part of the texture.\n\nTo solve this issue WebGL uses \n\nmipmaps (mip is short for the latin phrase “multum in parvo” or “much in little”) which are a series of textures, each one half the size of the previous one. WebGL will use a mipmap texture most suitable based on the distance of the fragment from the viewer. This way the fragment does not span a large part of the texture, and it also cuts down on memory.\n\n\n\nMipmaps\n\nThe good news is that we do not need to create lots of new different size textures because WebGL has a function glGenerateMipmap() to do this for us which we have been using for a while.\n\nOne issue we may encounter is that when we switch between two mipmaps, e.g., when the viewer is moving towards or away from an object, there can be a notable change in appearance of the object. This is known as texture popping and is caused by switching between two mipmaps. To overcome this WebGL gives the option to sample the texture from a linear interpolation between the two nearest mipmaps. So we have two main texture filtering options and two mipmap options giving four main mipmap options:\n\nGL_NEAREST_MIPMAP_NEAREST - uses nearest texture filtering on the nearest mipmap;\n\nGL_LINEAR_MIPMAP_NEAREST - uses linear texture filtering on the nearest mipmap;\n\nGL_NEAREST_MIPMAP_LINEAR - uses nearest texture filtering on a linear interpolation between two mipmaps;\n\nGL_LINEAR_MIPMAP_LINEAR - uses linear texture filtering on a linear interpolation between two mipmaps.\n\nLike with the texture filtering methods we can use different options for magnifying and minifying the texture. A popular combination is to use GL_LINEAR for the magnification filter and GL_LINEAR_MIPMAP_LINEAR for the minification filter to avoid textures popping as we zoom into a polygon.glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);\nglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);","type":"content","url":"/textures#mipmaps-section","position":31},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Multiple textures"},"type":"lvl2","url":"/textures#multiple-textures","position":32},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Multiple textures"},"content":"WebGL allows us to use multiple textures in a single fragment shader (up to 16 in fact). For each new texture we use we need to create and bind the texture to a target, load the texture data from an image file and set the texture wrapping and filtering options. Rather than copying and pasting all the code we have done for each new texture it makes sense to write a function that does this for us. Well if you look in the Lab03_Textures project, hidden away in the Header files folder is the file texture.hpp that contains a function loadTexture() that does all the hard work for us.\n\nComment out the code used to load the texture and specify the texture options and enter the following code before the render loop.// Load the textures\nunsigned int texture = loadTexture(\"../assets/crate.jpg\");\n\nThis creates a texture, loads in the data from the image file into the texture and sets the texture parameters. Compile and run the program, and you should see the image from \n\nFigure 7.\n\nWe want to work with more than one texture so replace the code above with the following.// Load the textures\nunsigned int texture1 = loadTexture(\"../assets/crate.jpg\");\nunsigned int texture2 = loadTexture(\"../assets/mario.png\");\n\nThis loads the crate and Mario textures and assigns them to the targets texture1 and texture2. We now want to deal with two textures in the fragment shader, so we need a way of telling WebGL which texture is which, we do this using uniforms.","type":"content","url":"/textures#multiple-textures","position":33},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Uniforms","lvl2":"Multiple textures"},"type":"lvl3","url":"/textures#uniforms-section","position":34},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Uniforms","lvl2":"Multiple textures"},"content":"A \n\nuniform is a shader variable that remains constant during the execution of the rendering pass and has the same value for all vertices and fragments. Uniforms provide a way to passing data to the shaders, so we will use one for passing the texture target to the fragment shader.\n\nAdd the following code to your program before the render loop (since the textures are the same for every frame).// Send the texture uniforms to the fragment shader\nglUseProgram(shaderID);\nglUniform1i(glGetUniformLocation(shaderID, \"texture1\"), 0);\nglUniform1i(glGetUniformLocation(shaderID, \"texture2\"), 1);\n\nHere we let WebGL know we are using our shader program with the glUseProgram() function. We then create uniforms for the two texture and assign them values of 0 and 1 using the glUniform1i() function. These values are the texture units that WebGL uses to distinguish between the different textures in the fragment shader.","type":"content","url":"/textures#uniforms-section","position":35},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture units","lvl2":"Multiple textures"},"type":"lvl3","url":"/textures#texture-units","position":36},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Texture units","lvl2":"Multiple textures"},"content":"A texture unit is a location value used by fragment shader for the texture sampler we are using. The default texture unit for a texture is GL_TEXTURE0 which is what we have been using up to now. We can have up to 16 texture units, GL_TEXTURE0, GL_TEXTURE1 up to GL_TEXTURE15. Alternatively we could use GL_TEXTURE0, GL_TEXTURE0 + 1, GL_TEXTURE0 + 2 up to GL_TEXTURE + 15.\n\nAdd the following code before the render loop.// Bind the textures\nglActiveTexture(GL_TEXTURE0);\nglBindTexture(GL_TEXTURE_2D, texture1);\nglActiveTexture(GL_TEXTURE1);\nglBindTexture(GL_TEXTURE_2D, texture2);\n\nThe glActiveTexture() function lets WebGL know what texture unit we are currently dealing with, and then we bind the texture to that unit using then glBindTexture() function.","type":"content","url":"/textures#texture-units","position":37},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Fragment shader","lvl2":"Multiple textures"},"type":"lvl3","url":"/textures#fragment-shader-1","position":38},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl3":"Fragment shader","lvl2":"Multiple textures"},"content":"The last thing we need to do is update the fragment shader so that it uses both textures. Modify fragmentShader.glsl, so that is looks like the following.#version 330 core\n\n// Input\nin vec2 UV;\n\n// Output\nout vec3 colour;\n\n// Uniforms\nuniform sampler2D texture1;\nuniform sampler2D texture2;\n\nvoid main()\n{\n    colour = vec3(mix(texture(texture1, UV), texture(texture2, UV), 0.7));\n}\n\nHere we have defined the two sampler2D uniforms texture1 and texture2, these need to be the same as what we called them in the glGetUniformLocation() functions. We then use the mix() function to combine the two textures so that 30% of the fragment colour is from the first texture (the crate) and the remaining 70% is from the second texture (Mario). Compile and run the program, and you should see the image shown in \n\nFigure 19.\n\n\n\nFigure 19:A rectangle with a mix of two textures applied.","type":"content","url":"/textures#fragment-shader-1","position":39},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Exercises"},"type":"lvl2","url":"/textures#exercises","position":40},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Exercises"},"content":"Change the uv array to create a texture rectangle consisting of a 6 by 4 grid of Mario’s.\n\n\n\nModify the fragment shader so that Mario is facing to the right instead of the left. Hint: the command vec2(vector.x, vector.y) creates a 2-element vector using the elements of vector.\n\n\n\nModify the fragment shader so that the red and green colour components of the pixel are switched.\n\n\n\nApply a texture of your choice to the rectangle (e.g., a selfie).\n\n\n\nChange the (u,v) coordinates so that the textured rectangle shows a zoomed in image of Mario’s eye.\n\n","type":"content","url":"/textures#exercises","position":41},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Video walkthrough"},"type":"lvl2","url":"/textures#video-walkthrough","position":42},{"hierarchy":{"lvl1":"Lab 3: Textures","lvl2":"Video walkthrough"},"content":"The video below walks you through these lab materials.","type":"content","url":"/textures#video-walkthrough","position":43},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices"},"type":"lvl1","url":"/vectors-and-matrices","position":0},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices"},"content":"Computer graphics relies heavily on mathematics of vectors and matrices. In this lab we will be revising the important concepts needed for computer graphics and using a library to perform calculations.\n\nIn this lab we will not be drawing any graphical objects, but we will be writing JavaScript code to perform calculations. So the first thing we are going to do is set up a simple HTML page and write a JavaScript function to print console output to the page.\n\nTask\n\nCreate a folder called 04 Vectors and Matrices inside which create a file called index.html and enter the following into it.<!doctype html>\n\n<html lang=\"en\">\n  <head>\n    <title>Lab 4 - Vectors and Matrices</title>\n  </head>\n  <body>\n    <div id=\"console-output\" \n         style=\"font-family:monospace; white-space: pre; padding:10px;\">\n    </div>\n    <script src=\"maths.js\"></script>\n    <script src=\"vectors_and_matrices.js\"></script>\n  </body>\n</html>\n\nCreate another file called vectors_and_matrices.js and enter the following into it.function setupConsoleOutput(elementId) {\n  const output = document.getElementById(elementId);\n\n  function write(args) {\n    const line = document.createElement(\"div\");\n    line.textContent = [...args].join(\" \");\n    output.appendChild(line);\n  }\n  console.log = (...args) => write(args);\n}\n\nsetupConsoleOutput(\"console-output\");\nconsole.log('Lab 4 - Vectors and Matrices\\n----------------------------');\n\nHere we have the function setupConsoleOutput() in the vectors_and_matrices.js file which means that any call to console.log() will output to the HTML page.Lab 4 - Vectors and Matrices\n----------------------------","type":"content","url":"/vectors-and-matrices","position":1},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Vectors"},"type":"lvl2","url":"/vectors-and-matrices#vectors","position":2},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Vectors"},"content":"A vector in is an object with magnitude (length) and direction. A vector is denoted by a lower case letter in boldface, e.g., \\vec{a} (or underlined when writing by hand), and represented mathematically by a tuple which is an ordered set of numbers. In geometry, each number in the vector represents the length along the co-ordinate axes. For example, consider the 3-element vector\\vec{a} = (a_x, a_y, a_z).\n\nHere \\vec{a} has 3 elements so is a vector in 3D space where a_x, a_y and a_z are the lengths of the vector in the x, y, and z directions.\n\n\n\nFigure 1:A 3D vector.\n\nNote\n\nThe reason the diagram above has the y-axis pointing upwards and the z-axis pointing along the horizontal is because this is the way OpenGL represents 3D space (see \n\n5. Transformations for more details). The configuration of the axes does not matter for the calculations we will be performing in this lab, but I wanted to be consistent.\n\nSince we will be using vectors (and matrices) a lot over the rest of the labs we will create a vector class to define vectors and perform operations on them.\n\nTask\n\nCreate file called maths.js and enter the following class definition.// 3-element vector class\nclass Vec3 {\n\n  constructor(x = 0, y = 0, z = 0) {\n    this.x = x;\n    this.y = y;\n    this.z = z;\n  }\n\n  // Print vector\n  print() {\n    return `[ ${this.x.toFixed(4)}, ${this.y.toFixed(4)}, ${this.z.toFixed(4)} ]`;\n  }\n}\n\nHere we have declared a class called Vec3 inside which we have defined the constructor function and a function to print the vector. Now let’s create the following vector objects in JavaScript and print them.\\begin{align*}\n    \\vec{a} &= (3, 0, 4), &\n    \\vec{b} &= (1, 2, 3).\n\\end{align*}\n\nTask\n\nAdd the following code to the vectors_and_matrices.js file.// Define vector objects\nconsole.log('\\nVectors\\n-------');\nconst a = new Vec3(3, 0, 4)\nconst b = new Vec3(1, 2, 3);\nconsole.log(\"a = \", a.print());\nconsole.log(\"b = \", b.print());\n\nHere we have created two vector objects a and b that contain the elements of \\vec{a} and \\vec{b} and printed these to our webpage which should now look likeVectors\n-------\na =  [ 3.0000, 0.0000, 4.0000 ]\nb =  [ 1.0000, 2.0000, 3.0000 ]","type":"content","url":"/vectors-and-matrices#vectors","position":3},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Arithmetic operations on vectors"},"type":"lvl2","url":"/vectors-and-matrices#arithmetic-operations-on-vectors","position":4},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Arithmetic operations on vectors"},"content":"Like numbers, we can define the arithmetic operations of addition, subtraction for vectors as well as multiplication and division by a scalar.","type":"content","url":"/vectors-and-matrices#arithmetic-operations-on-vectors","position":5},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Vector addition and subtraction","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#vector-addition-and-subtraction","position":6},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Vector addition and subtraction","lvl2":"Arithmetic operations on vectors"},"content":"The addition and subtraction of two vectors \\vec{a} = (a_x, a_y, a_z) and \\vec{b} = (b_x, b_y, b_z) is defined by\\begin{align*}\n    \\vec{a} + \\vec{b} &= (a_x + b_x, a_y + b_y, a_z + b_z), \\\\\n    \\vec{a} - \\vec{b} &= (a_x - b_x, a_y - b_y, a_z - b_z).\n\\end{align*}\n\nFor example, given the vectors \\vec{a} = (3,0,4) and \\vec{b} = (1, 2, 3)\\begin{align*}\n    \\vec{a} + \\vec{b} &= (3, 0, 4) + (1, 2, 3) = (3 + 1, 0 + 2, 4 + 3) = (4, 2, 7), \\\\\n    \\vec{a} - \\vec{b} &= (3, 0, 4) - (1, 2, 3) = (3 - 1, 0 - 2, 4 - 3) = (2, -2, 1).\n\\end{align*}\n\nWhat is happening in a geometrical sense when we add and subtract vectors? Take a look at \n\nFigure 2, here the vector \\vec{b} has been added to the vector \\vec{a} where the tail of \\vec{b} is placed at the head of \\vec{a}. The resulting vector \\vec{a} + \\vec{b} points from the tail of \\vec{a} to the head of \\vec{b}.\n\n\n\nFigure 2:Vector addition.\n\nThe subtraction of the vector \\vec{b} does similar, but since \\vec{a} - \\vec{b} = \\vec{a} + (-1)\\vec{b} then the direction of \\vec{b} is reversed so \\vec{a} - \\vec{b} is the same as placing the tail of -\\vec{b} at the head of \\vec{a}.\n\n\n\nFigure 3:Vector subtraction.\n\nTo calculate the addition and subtraction of vectors we are going to write functions to do this.\n\nTask\n\nAdd the following functions to your Vec3 class.// Arithmetic operations\nadd(v) {\n  return new Vec3(this.x + v.x, this.y + v.y, this.z + v.z);\n}\n\nsubtract(v) {\n  return new Vec3(this.x - v.x, this.y - v.y, this.z - v.z);\n}\n\nHere we have defined two similar functions add() and subtract() that add and subtract two vectors (not surprisingly). Both functions return the this keyword so when we call these functions on a vector object it will change the values in the vector.\n\nTask\n\nAdd the following to the Vectors_and_matrices.js file.// Arithmetic operations on vectors\nconsole.log('\\nArithmetic operations on vectors\\n--------------------------------');\nconsole.log(\"a + b =\", a.add(b).print());\nconsole.log(\"a - b =\", a.subtract(b).print());\n\nRefresh your web page, and you should see the following has been added.Arithmetic operations on vectors\n--------------------------------\na + b = [ 4.0000, 2.0000, 7.0000 ]\na - b = [ 2.0000, -2.0000, 1.0000 ]","type":"content","url":"/vectors-and-matrices#vector-addition-and-subtraction","position":7},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Multiplication by a scalar","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#multiplication-by-a-scalar","position":8},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Multiplication by a scalar","lvl2":"Arithmetic operations on vectors"},"content":"Multiplication of a vector \\vec{a} = (a_x, a_y, a_z) by a scalar (a number) k are defined by\\begin{align*}\n    k \\vec{a} &= (ka_x, ka_y, ka_z), \\\\\n    \\frac{\\vec{a}}{k} &= \\left(\\frac{a_x}{k}, \\frac{a_y}{k}, \\frac{a_z}{k} \\right).\n\\end{align*}\n\nFor example, multiplying the vector \\vec{a} = (3, 0, 4) by the scalar 2 gives2\\vec{a} = 2(3,0,4) = (6, 0, 8).\n\nIf we wanted to divide by a scale k then we simply multiply by \\dfrac{1}{k}. For example, dividing the vector \\vec{b} = (1, 2, 3) by 3 gives\\frac{\\vec{b}}{3} = \\frac{1}{3} \\vec{b} = \\left( \\frac{1}{3}, \\frac{2}{3}, \\frac{3}{3} \\right) = (0.3333, 0.6667, 1).\n\nMultiplying a vector by a positive scalar has the effect of scaling the length of the vector. Multiplying by a negative scalar reverses the direction of the vector.\n\n\n\nTask\n\nAdd the following function definition to the vector class.scale(s) {\n  return new Vec3(this.x * s, this.y * s, this.z * s);\n}\n\nNow add the following to the vectors_and_matrices.js file.console.log(\"2a    =\", a.scale(2).print());\nconsole.log(\"b/3   =\", b.scale(1/3).print());\n\nRefresh your web page, and you should see the following has been added.2a    = [ 6.0000, 0.0000, 8.0000 ]\nb/3   = [ 0.3333, 0.6667, 1.0000 ]","type":"content","url":"/vectors-and-matrices#multiplication-by-a-scalar","position":9},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Vector magnitude","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#vector-magnitude-section","position":10},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Vector magnitude","lvl2":"Arithmetic operations on vectors"},"content":"The length or magnitude of a vector \\vec{a} = (a_x, a_y, a_z) is denoted by \\|\\vec{a}\\| is the length from the tail of the vector to the head.\n\n\n\nFigure 5:Vector magnitude (length).\n\nThe magnitude is calculated using an extension of Pythagoras’ theorem, for example for 3D vectors the magnitude is\\|\\vec{a}\\| = \\sqrt{a_x^2 + a_y^2 + a_z^2}.\n\nFor example, if \\vec{a} = (3, 0, 4) and \\vec{b} = (1, 2, 3) then their magnitudes are\\begin{align*}\n    \\| \\vec{a} \\| &= \\sqrt{3^2 + 0^2 + 4^2} = \\sqrt{9 + 0 + 16} = \\sqrt{25} = 5, \\\\\n    \\| \\vec{b} \\| &= \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{1 + 4 + 9} = \\sqrt{14} = 3.7416\\ldots\n\\end{align*}\n\nTask\n\nAdd the following function definition to the vector class.// Length and normalization\nlength() {\n  return Math.sqrt(this.x * this.x + this.y * this.y + this.z * this.z);\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.// Vector magnitude and normalization\nconsole.log(\"\\nVector magnitude and normalization\\n----------------------------------\");\nconsole.log(\"length(a)    =\",a.length());\nconsole.log(\"length(b)    =\",b.length());\n\nRefresh your web page, and you should see the following has been added.Vector magnitude and normalization\n----------------------------------\nlength(a)    = 5\nlength(b)    = 3.7416573867739413","type":"content","url":"/vectors-and-matrices#vector-magnitude-section","position":11},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Unit vectors","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#unit-vectors-section","position":12},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Unit vectors","lvl2":"Arithmetic operations on vectors"},"content":"A unit vector is a vector that has a length of 1. We can find a unit vector that points in the same direction as a non-zero vector \\vec{a}, which is denoted by \\hat{a} (pronounced a-hat), by dividing by its magnitude, i.e.,\\hat{a} = \\frac{\\vec{a}}{\\|\\vec{a}\\|}.\n\nThis process is called normalising a vector. For example, to determine a unit vector pointing in the same direction as the vector \\vec{a} = (3, 0, 4), we normalize it by dividing by its magnitude which is 5.\\begin{align*}\n    \\hat{a} &= \\frac{(3, 0, 4)}{5} = \\left( \\frac{3}{5}, 0, \\frac{4}{5} \\right) = (0.6, 0, 0.8).\n\\end{align*}\n\nChecking that \\hat{a} has a magnitude of 1\\|\\hat{a}\\| = \\sqrt{0.6^2 + 0^2 + 0.8^2} = \\sqrt{0.36 + 0.64} = \\sqrt{1} = 1.\n\nNormalising a vector is an operation that is used a lot in graphics programming, so it would be useful to have a function that does this.\n\nTask\n\nAdd the following function definition to the vector class.normalize() {\n  const len = this.length();\n  if (len === 0) return new Vec3(0, 0, 0);\n  const inv = 1 / len;\n  return new Vec3(this.x * inv, this.y * inv, this.z * inv);\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.const aHat = a.normalize();\nconst bHat = b.normalize();\nconsole.log(\"aHat         =\", aHat.print());\nconsole.log(\"bHat         =\", bHat.print());\nconsole.log(\"length(aHat) =\", aHat.length());\nconsole.log(\"length(bHat) =\", bHat.length());\n\nRefresh your web page, and you should see the following has been added.aHat         = [ 0.6000, 0.0000, 0.8000 ]\nbHat         = [ 0.2673, 0.5345, 0.8018 ]\nlength(aHat) = 1\nlength(bHat) = 1\n\nBoth aHat and bHat have magnitudes of 1 which shows they are both unit vectors.","type":"content","url":"/vectors-and-matrices#unit-vectors-section","position":13},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"The dot product","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#dot-product-section","position":14},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"The dot product","lvl2":"Arithmetic operations on vectors"},"content":"The \n\ndot product between two vectors \\vec{a} = (a_x, a_y, a_z) and \\vec{b} = (b_x, b_y, b_z) is denoted by \\vec{a} \\cdot \\vec{b} and returns a scalar. The dot product is calculated using\\vec{a} \\cdot \\vec{b} = a_xb_x + a_yb_y + a_zb_z.\n\nThe dot product is related to the angle \\theta between the two vectors (\n\nFigure 6) by\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta).\n\n\n\nFigure 6:The angle \\theta between the vectors \\vec{a} and \\vec{b}.\n\nA useful result for computer graphics is that if \\theta=90^\\circ then \\cos(\\theta) = 0 and equation \n\n(5) becomes\\vec{a} \\cdot \\vec{b} = 0.\n\nIn order words, if the dot product of two vectors is zero then the two vectors are perpendicular. For example, given the vectors \\vec{a} = (3, 0, 4) and \\vec{b} = (1, 2, 3) the dot product between these are\\begin{align*}\n    \\vec{a} \\cdot \\vec{b} &= (3, 0, 4) \\cdot (1, 2, 3)\n    = 3 + 0 + 12\n    = 15.\n\\end{align*}\n\nTask\n\nAdd the following function definition to the vector class.// Dot and cross products\nconsole.log(\"\\nDot and cross products\\n----------------------\");\nconsole.log(\"a . b       =\", a.dot(b));\n\nNow add enter the following code to the vectors_and_matrices.js file.// Dot and cross products\nconsole.log(\"\\nDot and cross products\\n----------------------\");\nconst aDotB = a.clone().dot(b);\nconsole.log(\"a . b       =\", aDotB);\n\nRefresh your web page, and you should see the following has been added.Dot and cross products\n----------------------\na . b = 15","type":"content","url":"/vectors-and-matrices#dot-product-section","position":15},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"The cross product","lvl2":"Arithmetic operations on vectors"},"type":"lvl3","url":"/vectors-and-matrices#cross-product-section","position":16},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"The cross product","lvl2":"Arithmetic operations on vectors"},"content":"The \n\ncross product between two 3-element vectors \\vec{a} = (a_x, a_y, a_z) and \\vec{b} = (b_x, b_y, b_z) is denoted by \\vec{a} \\times \\vec{b} and returns a vector. The cross product is calculated using\\vec{a} \\times \\vec{b} = (a_yb_z - a_zb_y, a_zb_x - a_xb_z, a_xb_y - a_yb_x).\n\nThe cross product between two vectors produces another vector that is perpendicular to both of the vectors (\n\nFigure 7). This is another incredibly useful result as it allows us to calculate a \n\nnormal vector to a polygon which are used in calculating how light is reflected off surfaces (see \n\nLab 8: Lighting).\n\n\n\nFigure 7:The cross product between two vectors gives a vector that is perpendicular to both vectors.\n\nFor example, given the vectors \\vec{a} = (3,0,4) and \\vec{b} = (1, 2, 3) the cross product \\vec{a} \\times \\vec{b} is\\begin{align*}\n    \\vec{a} \\times \\vec{b} &= (3, 0, 4) \\times (1, 2, 3) \\\\\n    &= (0 \\times 3 - 4 \\times 2, 4 \\times 1 - 3 \\times 3, 3 \\times 2 - 0 \\times 3) \\\\\n    &= (-8, -5, 6).\n\\end{align*}\n\nWe can show that \\vec{a} \\times \\vec{b} is perpendicular to both \\vec{a} and \\vec{b} using the dot product\\begin{align*}\n    \\vec{a} \\cdot (\\vec{a} \\times \\vec{b}) &= (3, 0, 4) \\cdot (-8, -5, 6) = -24 + 0 + 24 = 0, \\\\\n    \\vec{b} \\cdot (\\vec{a} \\times \\vec{b}) &= (1, 2, 3) \\cdot (-8, -5, 6) = - 8 - 10 + 18 = 0.\n\\end{align*}\n\nTask\n\nAdd the following function definition to the vector class.cross(v) {\n  return new Vec3(\n    this.y * v.z - this.z * v.y,\n    this.z * v.x - this.x * v.z,\n    this.x * v.y - this.y * v.x\n  )\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.const aCrossB = a.cross(b);\nconsole.log(\"a x b       =\", aCrossB.print());\nconsole.log(\"a . (a x b) =\", a.dot(aCrossB));\nconsole.log(\"b . (a x b) =\", b.dot(aCrossB));\n\nRefresh your web page, and you should see the following has been added.a x b       = [ -8.0000, -5.0000, 6.0000 ]\na . (a x b) = 0\nb . (a x b) = 0\n\nHere we have also shown that the cross product of a and b is perpendicular to both vectors.","type":"content","url":"/vectors-and-matrices#cross-product-section","position":17},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Matrices"},"type":"lvl2","url":"/vectors-and-matrices#matrices","position":18},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Matrices"},"content":"Another type of mathematic object that is fundamental to computer graphics is the matrix. A matrix is a rectangular array of numbers.\\begin{align*}\n    A =\n    \\begin{pmatrix}\n        a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n        a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        a_{m1} & a_{m2} & \\cdots & a_{mn}\n    \\end{pmatrix}\n\\end{align*}\n\nIt is common to use uppercase characters for the name of a matrix and lowercase characters for the individual elements. The elements of a matrix are referenced by an index which is a pair of numbers, the first of which is the horizontal row number and the second is the vertical column number so a_{ij} is the element in row i and column j of the matrix A.\n\nWe refer to the size of a matrix by the number of rows by the number of columns. Here the matrix A has m rows and n columns, so we call this matrix a m \\times n matrix. Computer graphics mostly works with 4 \\times 4 matrices (see \n\nLab 5: Transformations for why this is) so we will create a matrix class to define 4 \\times 4 matrices and perform operations on them.\n\nTask\n\nAdd the following class declaration to the maths.js file.// 4x4 Matrix class\nclass Mat4 {\n  constructor() {\n    this.m = new Float32Array(16);\n  }\n\n  // Print matrix\n  print() {\n    let string = \"\";\n    for (let i = 0; i < 4; i++) {\n      const row = [\n        this.m[i * 4 + 0].toFixed(4),\n        this.m[i * 4 + 1].toFixed(4),\n        this.m[i * 4 + 2].toFixed(4),\n        this.m[i * 4 + 3].toFixed(4),\n      ];\n      string += \"  [ \" + row.join(\"  \") + \" ]\\n\";\n    }\n    return string;\n  }\n  \n  // Set\n  set(...values) {\n    if (values.length !== 16) {\n      throw new Error(\"Mat4.set() requires 16 values\");\n    }\n    for (let i = 0; i < 16; i++) {\n      this.m[i] = values[i];\n    }\n    return this;\n  }\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.// Matrices\nconsole.log(\"\\nMatrices\\n--------\");\nconst A = new Mat4().set(\n   1,  2,  3,  4,\n   5,  6,  7,  8,\n   9, 10, 11, 12,\n  13, 14, 15, 16\n);\nconsole.log(\"A =\\n\", A.print());\n\nHere we have declared a class called Mat4 inside which we have defined the constructor() function that defines a 4\\times 4 matrix of zeros, a print() function, a set() function that sets the elements of a matrix to values from a 16-element array and a clone() function. We have then created a matrix object and set the values equal toA =\n\\begin{pmatrix}\n  1 & 2 & 3 & 4 \\\\\n  5 & 6 & 7 & 8 \\\\\n  9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix}\n\nAnd printed the matrix. Refresh your web page, and you should see the following has been added.Matrices\n--------\nA =\n   [ 1.0000  2.0000  3.0000  4.0000 ]\n  [ 5.0000  6.0000  7.0000  8.0000 ]\n  [ 9.0000  10.0000  11.0000  12.0000 ]\n  [ 13.0000  14.0000  15.0000  16.0000 ]","type":"content","url":"/vectors-and-matrices#matrices","position":19},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Matrix transpose","lvl2":"Matrices"},"type":"lvl3","url":"/vectors-and-matrices#transpose-section","position":20},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Matrix transpose","lvl2":"Matrices"},"content":"The transpose of a matrix A is denoted by A^\\mathsf{T} and is definedA_{ij}^\\mathsf{T} = A_{ji}\n\ni.e., the rows and columns of A are swapped so row i of A is column i of A^\\mathsf{T}. For example, the matrix A we defined aboveA =\n\\begin{pmatrix}\n  1 & 2 & 3 & 4 \\\\\n  5 & 6 & 7 & 8 \\\\\n  9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix},\n\nthen A^\\mathsf{T} isA^\\mathsf{T} =\n\\begin{pmatrix}\n  1 & 5 & 9 & 13 \\\\\n  2 & 6 & 10 & 14 \\\\\n  3 & 7 & 11 & 15 \\\\\n  4 & 8 & 12 & 16\n\\end{pmatrix}.\n\nTask\n\nAdd the following function definition to the matrix class.// Arithmetic operations\ntranspose() {\n  let m = this.m;\n  return new Mat4().set(\n    m[0], m[4], m[8],  m[12],\n    m[1], m[5], m[6],  m[13],\n    m[2], m[6], m[10], m[14],\n    m[3], m[7], m[11], m[15]\n  );\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.console.log(\"\\nA^T =\\n\", A.transpose().print());\n\nRefresh your web page, and you should see the following has been added.A^T =\n   [ 1.0000  5.0000  9.0000  13.0000 ]\n  [ 2.0000  6.0000  10.0000  14.0000 ]\n  [ 3.0000  7.0000  11.0000  15.0000 ]\n  [ 4.0000  8.0000  12.0000  16.0000 ]","type":"content","url":"/vectors-and-matrices#transpose-section","position":21},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Matrix multiplication","lvl2":"Matrices"},"type":"lvl3","url":"/vectors-and-matrices#matrix-multiplication-section","position":22},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Matrix multiplication","lvl2":"Matrices"},"content":"Scalar multiplication of a matrix by a scalar is the same for matrices as it is for vectors. However, the multiplication of two matrices A and B is defined in a very specific way. If A and B are two matrices then the element in row i and column j of the matrix AB is calculated using[AB]_{ij} = \\vec{a}_i \\cdot \\vec{b}_j,\n\nWhere \\vec{a}_i is the vector formed from row i of A and \\vec{b}_j is the vector formed from column j of B. In computer graphics we mainly work with 4 \\times 4 matrices, so consider the following matrix multiplication\\begin{pmatrix}\n  1 & 2 & 3 & 4 \\\\\n  5 & 6 & 7 & 8 \\\\\n  9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix}\n\\begin{pmatrix}\n  17 & 18 & 19 & 20 \\\\\n  21 & 22 & 23 & 24 \\\\\n  25 & 26 & 27 & 28 \\\\\n  29 & 30 & 31 & 32\n\\end{pmatrix}\n\nFor the element in row 2 and column 3, [AB]_{23}, we have the dot product between row 2 of the left-hand matrix and column 3 of the right-hand matrix\\begin{pmatrix}\n  \\color{lightgray}1 & \\color{lightgray}2 & \\color{lightgray}3 & \\color{lightgray}4 \\\\\n  5 & 6 & 7 & 8 \\\\\n  \\color{lightgray}9 & \\color{lightgray}10 & \\color{lightgray}11 & \\color{lightgray}12 \\\\\n  \\color{lightgray}13 & \\color{lightgray}14 & \\color{lightgray}15 & \\color{lightgray}16\n\\end{pmatrix}\n\\begin{pmatrix}\n  \\color{lightgray}17 & \\color{lightgray}18 & 19 & \\color{lightgray}20 \\\\\n  \\color{lightgray}21 & \\color{lightgray}22 & 23 & \\color{lightgray}24 \\\\\n  \\color{lightgray}25 & \\color{lightgray}26 & 27 & \\color{lightgray}28 \\\\\n  \\color{lightgray}29 & \\color{lightgray}30 & 31 & \\color{lightgray}32\n\\end{pmatrix}\n\nso(5, 6, 7, 8) \\cdot (19, 23, 27, 31) = 5 \\times 19 + 6 \\times 23 + 7 \\times 27 + 8 \\times 31 = 670.\n\nDoing similar for the other elements gives\\begin{pmatrix}\n  1 & 2 & 3 & 4 \\\\\n  5 & 6 & 7 & 8 \\\\\n  9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix}\n\\begin{pmatrix}\n  17 & 18 & 19 & 20 \\\\\n  21 & 22 & 23 & 24 \\\\\n  25 & 26 & 27 & 28 \\\\\n  29 & 30 & 31 & 32\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n   250 &  260 &  270 &  280 \\\\\n   618 &  644 &  670 &  696 \\\\\n   986 & 1028 & 1070 & 1112 \\\\\n  1354 & 1412 & 1470 & 1528 \\\\\n\\end{pmatrix}\n\nTask\n\nAdd the following function definition to the matrix class.multiply(mat) {\n  const c = new Float32Array(16);\n  for (let col = 0; col < 4; col++) {\n    for (let row = 0; row < 4; row++) {\n      for (let i  = 0; i < 4; i++) {\n        c[col * 4 + row] += this.m[i * 4 + row] * mat.m[col * 4 + i];\n      }\n    }\n  }\n  return new Mat4().set(...c);\n}\n\nNow add enter the following code to the vectors_and_matrices.js file.const B = new Mat4().set(\n  17, 18, 19, 20,\n  21, 22, 23, 24,\n  25, 26, 27, 28,\n  29, 30, 31, 32\n);\nconsole.log(\"\\nB =\\n\", B.print());\nconsole.log(\"\\nAB =\\n\", A.multiply(B).print());\n\nRefresh your web page, and you should see the following has been added.B =\n   [ 17.0000  18.0000  19.0000  20.0000 ]\n  [ 21.0000  22.0000  23.0000  24.0000 ]\n  [ 25.0000  26.0000  27.0000  28.0000 ]\n  [ 29.0000  30.0000  31.0000  32.0000 ]\n\nAB =\n   [ 538.0000  612.0000  686.0000  760.0000 ]\n  [ 650.0000  740.0000  830.0000  920.0000 ]\n  [ 762.0000  868.0000  974.0000  1080.0000 ]\n  [ 874.0000  996.0000  1118.0000  1240.0000 ]\n\nWhat... wait... hang on a minute, this matrix isn’t the same as the one from equation \n\n(11). Our .multiply() function hasn’t given us the result shown above. The reason for this is something called column-major order.","type":"content","url":"/vectors-and-matrices#matrix-multiplication-section","position":23},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Column-major order","lvl2":"Matrices"},"type":"lvl3","url":"/vectors-and-matrices#column-major-order-section","position":24},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl3":"Column-major order","lvl2":"Matrices"},"content":"Linear memory is a contiguous block of addresses that can be sequentially accessed. So a 1D array is stored in adjacent memory locations. Since matrices are 2D we have a choice whether to store the elements in the rows or columns in adjacent locations. These are known as column-major order and row-major order. Consider the 4 \\times 4 matrix\\begin{pmatrix}\n    a & b & c & d \\\\\n    e & f & g & h \\\\\n    i & j & k & l \\\\\n    m & n & o & p\n\\end{pmatrix}.\n\nUsing column-major order this will be stored in the memory as\n\n\n\ni.e., we move down and across the matrix. Alternatively, using row-major order the matrix will be stored as\n\n\n\ni.e., we move across and down the matrix. WebGL uses column-major order because it is based upon OpenGL which was written for early GPUs that treated vertex data as column vectors. So a matrix containing vertices is stored column-by-column which means, when working with WebGL, we need to switch the rows and columns around when multiplying matrices. This is why our .multiply() function produced the wrong result.\n\nTo output the matrix multiplication AB as we would expect it to appear, we can swap A and B.\n\nTask\n\nEdit the last line you entered so the A and B are swapped.console.log(\"\\nAB =\\n\", B.multiply(A).print());\n\nRefresh your browser and you should now see that we have the matrix seen in equation .AB =\n   [ 250.0000  260.0000  270.0000  280.0000 ]\n  [ 618.0000  644.0000  670.0000  696.0000 ]\n  [ 986.0000  1028.0000  1070.0000  1112.0000 ]\n  [ 1354.0000  1412.0000  1470.0000  1528.0000 ]\n\nNote\n\nMicrosoft’s graphics library directX and Unreal Engine uses row-major order whilst WebGL, OpenGL, Vulkan (successor to OpenGL), Metal (Apple’s graphics library) and Unity all use column-major order. This means when porting code between the graphics libraries developers have to change all of their matrix calculations. ---\n\n## The Identity Matrix\n\nThe identity matrix is a special square matrix where all the elements are zero apart from the elements on the diagonal line from the top-left element down to the bottom-right element (known as the main diagonal). For example the $4 \\times 4$ identity matrix is\n\n```{math}\nI =\n\\begin{pmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 1\n\\end{pmatrix}.\n```\n\nThe identity matrix is similar to the number 1 in that if we multiply any matrix by an identity matrix the result is unchanged. For example,\n\n```{math}\nIA =\n\\begin{pmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 \\\\\n  0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n   1 &  2 &  3 &  4 \\\\\n   5 &  6 &  7 &  8 \\\\\n   9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n   1 &  2 &  3 &  4 \\\\\n   5 &  6 &  7 &  8 \\\\\n   9 & 10 & 11 & 12 \\\\\n  13 & 14 & 15 & 16\n\\end{pmatrix}\n= A.\n```\n\n:::{admonition} Task\n:class: tip\n\nAdd the following function to the matrix class.\n\n```javascript\n// Identity matrix\nidentity() {\n  this.m[0]  = 1;\n  this.m[5]  = 1;\n  this.m[10] = 1;\n  this.m[15] = 1;\n  return this;\n}\n```\n\nAnd add the following to the ***Vectors_and_matrices.js*** file.\n\n```javascript\nconst I = new Mat4().identity();\nconsole.log(\"I =\\n\", I.print());\n```\n\n:::\n\nRefresh your browser and you should now see that we have the matrix seen in equation {eq}`matrix-multiplication-example`.\n\n```text\nI =\n   [ 1.0000  0.0000  0.0000  0.0000 ]\n  [ 0.0000  1.0000  0.0000  0.0000 ]\n  [ 0.0000  0.0000  1.0000  0.0000 ]\n  [ 0.0000  0.0000  0.0000  1.0000 ]\n``` ","type":"content","url":"/vectors-and-matrices#column-major-order-section","position":25},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Exercises"},"type":"lvl2","url":"/vectors-and-matrices#vectors-exercises","position":26},{"hierarchy":{"lvl1":"Lab 4: Vectors and Matrices","lvl2":"Exercises"},"content":"Three points have the co-ordinates A = (5, 1, 3), B = (10, 7, 4) and C = (0, 5, -3). Use pen and paper to calculate the following:\n\n(a) The vector \\vec{p} that points from A to B;\n(b) The vector \\vec{q} that points from B to C;\n(c) The vector \\vec{r} that points from C to A;\n(d) The length of the vector \\vec{p};\n(e) A unit vector that points in the direction of the vector \\vec{q};\n(f) The dot product \\vec{p} \\cdot \\vec{q};\n(g) The cross product \\vec{q} \\times \\vec{r}.\n\nRepeat exercise 1 using your functions from the maths.js file.\n\nThe three matrices A, B and C are defined by\\begin{align*}\n    A &= \\begin{pmatrix} -1 & 3 \\\\ 2 & -5 \\end{pmatrix}, &\n    B &= \\begin{pmatrix} 0 & 2 \\\\ 7 & 1 \\end{pmatrix}, &\n    C &= \\begin{pmatrix} 3 & 2 \\\\ -3 & -4 \\end{pmatrix}.\n\\end{align*}\n\n   Use pen and paper to calculate the following:\n\n   (a) AB;\n   (b) ABC;\n   (c) B^\\mathsf{T}A^\\mathsf{T}.\n\nA transformation can be applied to a vector by matrix multiplication. If T is a transformation matrix and \\vec{v} is a vector then the transformed vector is T \\vec{v}. Given the following transformation matrices and vector\\begin{align*}\n  S &= \\begin{pmatrix}\n    2 & 0 & 0 & 0 \\\\\n    0 & 2 & 0 & 0 \\\\\n    0 & 0 & 2 & 0 \\\\\n    0 & 0 & 0 & 1\n  \\end{pmatrix}, &\n  T &= \\begin{pmatrix}\n    1 & 0 & 0 & 3 \\\\\n    0 & 1 & 0 & 2 \\\\\n    0 & 0 & 1 & -1 \\\\\n    0 & 0 & 0 & 1\n  \\end{pmatrix}, &\n  \\vec{v} = \\begin{pmatrix} 5 \\\\ 8 \\\\ 10 \\\\ 1 \\end{pmatrix},\n\\end{align*}\n\n   use pen and paper to calculate the following transformations:\n\n   (a)   S \\, \\vec{v};\n   (b)   T \\, \\vec{v};\n   (c)   T\\,S\\,\\vec{v}.\n\n   For each one, describe what effect the transformation has on \\vec{v}.\n\nSolutions\n\n(a)   \\vec{p} = (5, 6, 1) \n(b)   \\vec{q} = (-10, -2, -7) \n(c)   \\vec{r} = (5, -4, 6) \n(d)   \\| \\vec{p} \\| = 7.8740 \n(e)   \\hat{q} = (-0.8085,   -0.1617,   -0.5659) \n(f)   \\vec{p} \\cdot \\vec{q} = -69 \n(g)   \\vec{q} \\times \\vec{r} = (-40, 25, 50) \n\n// Exercise 2\nconsole.log(\"\\nExercise 2\\n----------\");\n\nconst A1 = new Float32Array([5, 1, 3]);\nconst B1 = new Float32Array([10, 7, 4]);\nconst C1 = new Float32Array([0, 5, -3]);\n\np = subtractVectors(B1, A1);\nprintVector(p, \"(a) p\");\nq = subtractVectors(C1, B1);\nprintVector(q, \"(b) q\");\nr = subtractVectors(A1, C1);\nprintVector(r, \"(c) r\");\nconsole.log(\"(d) length(p) = \" + length(p));\nprintVector(normalize(q), \"(e) qHat\")\nconsole.log(\"(f) p . q = \" + dot(p, q));\nprintVector(cross(q, r), \"(g) q x r\")Exercise 2\n----------\n(a) p = [ 5.0000, 6.0000, 1.0000 ]\n(b) q = [ -10.0000, -2.0000, -7.0000 ]\n(c) r = [ 5.0000, -4.0000, 6.0000 ]\n(d) length(p) = 7.874007874011811\n(e) qHat = [ -0.8085, -0.1617, -0.5659 ]\n(f) p . q = -69\n(g) q x r = [ -40.0000, 25.0000, 50.0000 ]\n\n(a)   AB = \\begin{pmatrix} 21 & 1 \\\\ -35 & -1 \\end{pmatrix}\n(b)   ABC = \\begin{pmatrix} 60 & 38 \\\\ -102 & -66 \\end{pmatrix}\n(c)   B^\\textsf{T} A^\\textsf{T} = \\begin{pmatrix} 21 & -35 \\\\ 1 & -1 \\end{pmatrix}\n\n(a)   S \\vec{v} = \\begin{pmatrix} 10 \\\\ 16 \\\\ 20 \\\\ 1 \\end{pmatrix}\n  The first three elements of \\vec{v} have been scaled up by a factor of 2, i.e., \\begin{pmatrix} 2 \\times 10 \\\\ 2 \\times 16 \\\\ 2 \\times 20 \\\\ 1 \\end{pmatrix}.\n(b)   T \\vec{v} = \\begin{pmatrix} 8 \\\\ 10 \\\\ 9 \\\\ 1 \\end{pmatrix}\n  The first three elements of \\vec{v} have been increased by 3, 2 and -1 respectively, i.e., \\begin{pmatrix} 5 + 3 \\\\ 8 + 2 \\\\ 10 - 1 \\\\ 1 \\end{pmatrix}.\n(c)   T \\, S \\, \\vec{v} = \\begin{pmatrix} 13 \\\\ 18 \\\\ 19 \\\\ 1 \\end{pmatrix}\n  The first three elements of \\vec{v} have been scaled up by a factor or 2 and then increased by 3, 2 and -1 respectively, i.e., \\begin{pmatrix} 2 \\times 5 + 3 \\\\ 2 \\times 8 + 2 \\\\ 2 \\times 10 - 1 \\\\ 1\\end{pmatrix}.","type":"content","url":"/vectors-and-matrices#vectors-exercises","position":27},{"hierarchy":{"lvl1":"Lab 5: Transformations"},"type":"lvl1","url":"/transformations","position":0},{"hierarchy":{"lvl1":"Lab 5: Transformations"},"content":"Computer graphics requires that shapes are manipulated in space by moving the shapes, shrinking or stretching, rotating and reflection to name a few. We call these manipulations transformations. We need a convenient way of telling the computer how to apply our transformations and for this we make use of matrices which we covered in \n\nLab 4: Vectors and Matrices.\n\nEach transformation has an associated transformation matrix which we use to multiply the vertex coordinates of a shape to calculate the vertex coordinates of the transformed shape. For example if A is a transformation matrix for a particular transformation and (x,y,z) are the coordinates of a vertex then we apply the transformation using\\begin{pmatrix} x' \\\\ y' \\\\ x' \\end{pmatrix} = A \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix},\n\nwhere (x',y',z') are the coordinates of the transformed point. Note that all vectors and coordinates are written as a column vector when multiplying by a matrix.\n\nTask\n\nCreate a folder called 05 Transformations and download \n\nindex.html, \n\ntransformations.js and \n\nwebGLUtils.js to it. Open index.html in a web browser to check that the red triangle from \n\nLab 3: Textures is displayed.\n\n","type":"content","url":"/transformations","position":1},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"WebGL coordinate system"},"type":"lvl2","url":"/transformations#webgl-coordinate-system","position":2},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"WebGL coordinate system"},"content":"In 3D graphics a coordinate system defines how points, directions and rotations are represented. The two main conventions are the right-handed and left-handed coordinates systems. Both use x, y and z axes but they differ in which direction the z-axis points relative to the x and y axis.\n\nThe right-handed coordinate system is where on your right hand:\n\nthe thumb points along the positive x-axis,\n\nthe index finger points along the positive y-axis,\n\nthe middle finger points along the positive z-axis.\n\nThe other way of representing 3D space is to use a left-hand coordinate system which is the same but on your left hand.\n\nWebGL uses the right-handed coordinate system where the x-axis points to the right of the screen, the y-axis points towards the top of the screen and the z-axis points out of the screen towards you (\n\nFigure 2).\n\n\n\nFigure 2:The WebGL co-ordinate system.\n\nOther graphics libraries that use the right-handed coordinate system include OpenGL, Three.js, Vulkan, Metal (Apple) and applications such as Unreal Engine and Blender. Graphics libraies that use the left-handed coordinate system include DirectX, Direct3D and Unity.","type":"content","url":"/transformations#webgl-coordinate-system","position":3},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Translation"},"type":"lvl2","url":"/transformations#translation-section","position":4},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Translation"},"content":"The translation transformation when applied to a set of points moves each point by the same amount. For example, consider the triangle in \n\nFigure 3, each of the vertices has been translated by the same vector \\vec{t} which has that effect of moving the triangle.\n\n\n\nFigure 3:Translation of a triangle by the translation vector \\vec{t}= (t_x, t_y, t_z).\n\nA problem we have is that no transformation matrix exists for applying translation to the coordinates (x, y, z), i.e., we can’t find a matrix Translate such thatTranslate \\cdot \\begin{pmatrix} x  \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} x + t_x \\\\ y + t_y \\\\ z + t_z \\end{pmatrix}.\n\nWe can use a trick where we use \n\nhomogeneous coordinates. Homogeneous coordinates add another value, w say, to the (x, y, z) coordinates (known as Cartesian coordinates) such that when the x, y and z values are divided by w we get the Cartesian coordinates.\\underset{\\textsf{homogeneous}}{\\begin{pmatrix} x \\\\ y \\\\ z \\\\ w \\end{pmatrix}} \\equiv \\underset{\\textsf{Cartesian}}{\\begin{pmatrix} x/w \\\\ y/w \\\\ z/w \\end{pmatrix}}.\n\nSo if we choose w=1 then we can write the Cartesian coordinates (x, y, z) as the homogeneous coordinates (x, y, z, 1) (remember that 4-element vector with the additional 1 in our \n\nvertex shader?). So how does that help us with our elusive translation matrix? Well we can now represent translation as a 4 \\times 4 matrix\\begin{pmatrix}\n    1 & 0 & 0 & t_x \\\\\n    0 & 1 & 0 & t_y \\\\\n    0 & 0 & 1 & t_z \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}\n=\n\\begin{pmatrix} x + t_x \\\\ y + t_y \\\\ z + t_z \\\\ 1 \\end{pmatrix},\n\nwhich is our desired translation. So the translation matrix for translating a set of points by the vector \\vec{t} = (t_x, t_y, t_z) isTranslate = \\begin{pmatrix}\n    1 & 0 & 0 & t_x \\\\\n    0 & 1 & 0 & t_y \\\\\n    0 & 0 & 1 & t_z \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nImportant\n\nRecall that WebGL and glm use \n\ncolumn-major order, so when coding transformation matrices into JavaScript we need to code the transpose of the matrix. So the translation matrix we are going to use isTranslate = \\begin{pmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    t_x & t_y & t_z & 1\n\\end{pmatrix}\n\nLet’s translate the rectangle 0.4 to the right and 0.3 upwards (remember we are dealing with normalised device coordinates, so the window coordinates are between -1 and 1). The transposed transformation matrix to perform this translation isTranslate = \\begin{pmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0.4 & 0.3 & 0 & 1\n\\end{pmatrix}.\n\nWe are going to define matrix class to compute the various transformation matrices.\n\nTask\n\nAdd the following function definition to the matrix class in the maths.js file.// Transformation matrices\ntranslate(x, y, z) {\n  return new Mat4().set(\n    1, 0, 0, 0,\n    0, 1, 0, 0,\n    0, 0, 1, 0,\n    x, y, z, 1\n  );\n}\n\nAnd add the following to the translations.js file before we draw the triangles.// Calculate transformation matrices\nconst translate = new Mat4().translate(0.4, 0.3, 0);\n\nHere we have defined the function translate() that results the translation matrix for a given translation vector and have called this function to compute translateMatrix.\n\nThe multiplication of the vertex coordinates by the transformation matrices is done in the GPU as opposed to the CPU. This is because GPUs are specifically designed to perform matrix multiplication on millions of vertices in parallel, so doing this in the GPU is much faster and frees up the CPU. So we send the transformation matrix to the vertex shader using a  uniform, like we did in the lab on \n\ntexture maps.\n\nTask\n\nAdd the following code after we have calculated the translation matrix.// Calculate transformation matrix and send it to the shader\nconst model = translate;\ngl.uniformMatrix4fv(gl.getUniformLocation(shaderProgram, \"uModel\"), false, model.m);\n\nHere we have created another matrix called model and have sent this to the shader using the uniform name uModel. We will be applying multiple transformations to our object vertices. Rather then sending multiple matrices to the shaders, we multiply them in the CPU and send a single 4 \\times 4 matrix to the shaders. The model matrix is the combination of transformations that are applied to each vertex of the object.\n\nWe now have to do is modify the vertex shader to use our new transformation matrix.\n\nTask\n\nIn the vertex shader definition at the top of the transformations.js file, add the following uniform declaration before the main() function.uniform mat4 uModel;\n\nAnd change the calculation of gl_Position to the followinggl_Position = uModel * vec4(aPosition, 1.0);\n\nRefresh your web browser and you should see that our rectangle has been translated to the right and up a bit as shown in \n\nFigure 4.\n\n\n\nFigure 4:The rectangle is translated by the vector (0.4, 0.3, 0).","type":"content","url":"/transformations#translation-section","position":5},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Scaling"},"type":"lvl2","url":"/transformations#scaling","position":6},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Scaling"},"content":"Scaling is one of the simplest transformation we can apply. Multiplying the x, y and z coordinates of a point by a scalar quantity (a number) has the effect of moving the point closer or further away from the origin (0,0). For example, consider the triangle in \n\nFigure 5. The x, y and z coordinates of each vertex has been multiplied by s_x, s_y and s_y respectively which has the effect of scaling the triangle and moving the vertices further away from the origin (in this case because s_x, s_y and s_z are all greater than 1).\n\n\n\nFigure 5:Scaling a triangle centred at the origin.\n\nSince scaling is simply multiplying the coordinates by a number we have\\begin{align*}\n    \\begin{pmatrix}\n        s_x & 0 & 0 & 0 \\\\\n        0 & s_y & 0 & 0 \\\\\n        0 & 0 & s_z & 0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}\n    \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}\n    =\n    \\begin{pmatrix} s_xx \\\\ s_yy \\\\ s_zz \\\\ 1 \\end{pmatrix},\n\\end{align*}\n\nso the scaling matrix for applying the scaling transformation isScale =\n\\begin{pmatrix}\n    s_x & 0 & 0 & 0 \\\\\n    0 & s_y & 0 & 0 \\\\\n    0 & 0 & s_z & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nLet’s now apply scaling to our rectangle in WebGL to increase its size by a factor of 0.5 in the horizontal direction and 0.4 in the vertical direction. The scaling matrix that achieves this isScale = \\begin{pmatrix}\n    0.5 & 0 & 0 & 0 \\\\\n    0 & 0.4 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nWe have already created a model matrix and the uniform in the vertex shader, so we just need to calculate the scaling matrix and use it instead of the translation matrix.\n\nTask\n\nEnter the following function definition to the matrix class.scale(x, y, z) {\n  return new Mat4().set(\n    x, 0, 0, 0,\n    0, y, 0, 0,\n    0, 0, z, 0,\n    0, 0, 0, 1\n  );\n}\n\nEnter the following code to the tranformations.js file after we calcuate the translation matrix.const scale     = new Mat4().scale(0.5, 0.4, 1);\n\nAnd change the model matrix to the following.const model = scale;\n\nRefresh your web browser and you should see that our rectangle has been scaled down as shown in \n\nFigure 6.\n\n\n\nFigure 6:The rectangle is scaled by the vector (0.5, 0.4, 1).\n\nNote\n\nIf scaling is applied to a shape that is not centred at (0,0,0) then the transformed shape is distorted and its centre is moved from its original position (\n\nFigure 7).\n\n\n\nFigure 7:Scaling applied to a triangle not centred at (0,0,0).\n\nIf the desired result is to resize the shape whilst keeping its dimensions and location the same we first need to translate the vertex coordinates by -\\vec{c} where \\vec{c} is the centre of volume for the shape so that it is at (0,0,0). Then we can apply the scaling before translating by \\vec{c} so that the centre of volume is back at the original position (\n\nFigure 8).\n\n\n\nFigure 8:The steps required to scale a shape about its centre.","type":"content","url":"/transformations#scaling","position":7},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Rotation"},"type":"lvl2","url":"/transformations#rotation-section","position":8},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Rotation"},"content":"As well as translating and scaling objects, the next most common transformation is the rotation of objects around the three co-ordinate axes x, y and z. We define the rotation anti-clockwise around each of the co-ordinate axes by an angle \\theta when looking down the axes (\n\nFigure 9).\n\n\n\nFigure 9:Rotation is assumed to be in the anti-clockwise direction when looking down the axis.\n\nThe rotation matrices for achieving these rotations are\\begin{align*}\n    R_x &=\n    \\begin{pmatrix}\n        1 & 0 & 0 & 0 \\\\\n        0 & \\cos(\\theta) & -\\sin(\\theta) & 0 \\\\\n        0 & \\sin(\\theta) &  \\cos(\\theta) & 0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}, \\\\\n    R_y &=\n    \\begin{pmatrix}\n         \\cos(\\theta) & 0 & \\sin(\\theta) & 0 \\\\\n        0 & 1 & 0 & 0 \\\\\n        -\\sin(\\theta) & 0 & \\cos(\\theta) & 0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}, \\\\\n    R_z &=\n    \\begin{pmatrix}\n        \\cos(\\theta) & -\\sin(\\theta) & 0 & 0 \\\\\n        \\sin(\\theta) &  \\cos(\\theta) & 0 & 0 \\\\\n        0 & 0 & 1 & 0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}.\n\\end{align*}\n\nYou don’t really need to know how these are derived but if you are curious you can click on the dropdown link below.\n\nDerivation of the rotation matrices (click to show)\n\nWe will consider rotation about the z-axis and will restrict our coordinates to 2D.\n\n\n\nFigure 10:Rotating the vector \\vec{a} anti-clockwise by angle \\theta to the vector \\vec{b}.\n\nConsider \n\nFigure 10 where the vector \\vec{a} is rotated by angle \\theta to the vector \\vec{b}. If we form a right-angled triangle (the blue one) then we know the length of the hypotenuse, \\|\\vec{a}\\|, and the angle \\theta so we can calculate the lengths of the adjacent and opposite sides using trigonometry. Remember our trig ratios (SOH-CAH-TOA)\\begin{align*}\n    \\sin(\\phi) &= \\frac{opposite}{hypotenuse}, &\n    \\cos(\\phi) &= \\frac{adjacent}{hypotenuse}, &\n    \\tan(\\phi) &= \\frac{opposite}{adjacent},\n\\end{align*}\n\nso the length of the adjacent and opposite sides of the blue triangle is\\begin{align*}\n    adjacent &= hypotenuse \\cdot \\cos(\\phi), \\\\\n    opposite &= hypotenuse \\cdot \\sin(\\phi).\n\\end{align*}\n\nSince a_x and a_y are the lengths of the adjacent and opposite sides respectively and \\|\\vec{a}\\| is the length of the hypotenuse we have\\begin{align*}\n    a_x &= \\|\\vec{a}\\| \\cos(\\phi), \\\\\n    a_y &= \\|\\vec{a}\\| \\sin(\\phi).\n\\end{align*}\n\nUsing the same method for the vector \\vec{b} we have\\begin{align*}\n    b_x &= \\|\\vec{a}\\| \\cos(\\phi + \\theta), \\\\\n    b_y &= \\|\\vec{a}\\| \\sin(\\phi + \\theta).\n\\end{align*}\n\nWe can rewrite \\cos(\\phi+\\theta) and \\sin(\\phi+\\theta) using \n\ntrigonometric identities\\begin{align*}\n    \\cos(\\phi + \\theta) &= \\cos(\\phi) \\cos(\\theta) - \\sin(\\phi) \\sin(\\theta), \\\\\n    \\sin(\\phi + \\theta) &= \\sin(\\phi) \\cos(\\theta) + \\cos(\\phi) \\sin(\\theta),\n\\end{align*}\n\nso equation \n\n(15) is\\begin{align*}\n    b_x &= \\|\\vec{a}\\| \\cos(\\phi) \\cos(\\theta) - \\|\\vec{a}\\| \\sin(\\phi) \\sin(\\theta), \\\\\n    b_y &= \\|\\vec{a}\\| \\sin(\\phi) \\cos(\\theta) + \\|\\vec{a}\\| \\cos(\\phi) \\sin(\\theta).\n\\end{align*}\n\nSubstituting equation \n\n(14) into equation \n\n(17) gives\\begin{align*}\n    b_x &= a_x \\cos(\\theta) - a_y \\sin(\\theta), \\\\\n    b_y &= a_y \\sin(\\phi) + a_x \\sin(\\theta),\n\\end{align*}\n\nwhich can be written using matrices as\\begin{align*}\n    \\begin{pmatrix} b_x \\\\ b_y \\end{pmatrix} =\n    \\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}\n    \\begin{pmatrix} a_x \\\\ a_y \\end{pmatrix},\n\\end{align*}\n\nso the transformation (non-transposed) matrix for rotating around the z-axis in 2D is\\begin{pmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{pmatrix}.\n\nWe need a 4\\times 4 matrix to represent 3D rotation around the z-axis so we replace the 3rd and 4th row and columns with the 3rd and 4th row and column from the 4\\times 4 identity matrix givingR_z = \\begin{pmatrix}\n    \\cos(\\theta) & -\\sin(\\theta) & 0 & 0 \\\\\n    \\sin(\\theta) & \\cos(\\theta) & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nThe rotation matrices for the rotation around the x and y axes are derived using a similar process.\n\nLets rotate our original rectangle anti-clockwise about the z-axis by \\theta = 45^\\circ. The transposed rotation matrix to do this isRotate =\n\\begin{pmatrix}\n     \\cos(45^\\circ) & \\sin(45^\\circ) & 0 & 0 \\\\\n    -\\sin(45^\\circ) & \\cos(45^\\circ) & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nNote\n\nAngles in JavaScript are always expressed in radians so we need to use the following to convert from degrees to radiansradians = degrees \\times \\frac{\\pi}{180}\n\nTask\n\nEnter the following function definition to the matrix class.rotateZ(rad) {\n  const c = Math.cos(rad);\n  const s = Math.sin(rad);\n  return new Mat4().set(\n    c,  s, 0, 0,\n    -s, c, 0, 0,\n    0,  0, 1, 0,\n    0,  0, 0, 1\n  );\n}\n\nAdd the following to the transformations.js file after we calculate the scaling matrixconst angle     = 45 * Math.PI / 180;\nconst rotate    = new Mat4().rotate(0, 0, 1, angle);\n\nAnd change the model matrix to the following.const model = rotate;\n\nHere we defined a function to the matrix class to calculate the rotation matrix. Refresh your web browser and you should see that our rectangle has been rotated 45^\\circ degrees in the anti-clockwise direction as shown in \n\nFigure 11.\n\n\n\nFigure 11:Rectangle rotated anti-clockwise about the z-axis by 45^\\circ.","type":"content","url":"/transformations#rotation-section","position":9},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl3":"Axis-angle rotation","lvl2":"Rotation"},"type":"lvl3","url":"/transformations#axis-angle-rotation-section","position":10},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl3":"Axis-angle rotation","lvl2":"Rotation"},"content":"The three rotation transformations are only useful if we want to only rotate around one of the three co-ordinate axes. A more useful transformation is the rotation around the axis that points in the direction of a unit vector \\hat{v} which has its tail at (0,0,0) (\n\nFigure 12).\n\n\n\nFigure 12:Axis-angle rotation.\n\nThe transposed transformation matrix for rotation around a unit vector \\hat{v} = (v_x, v_y, v_z), anti-clockwise by angle \\theta when looking down the vector is.\\begin{align*}\n    Rotate =\n    \\begin{pmatrix}\n        (1 - c) v_x^2  + c &\n        (1 - c) v_x v_y + v_zs &\n        (1 - c) v_x v_z - v_ys &\n        0 \\\\\n        (1 - c) v_x v_y - v_zs &\n        (1 - c) v_y^2 + c &\n        (1 - c) v_y v_z + v_xs &\n        0 \\\\\n        (1 - c) v_x v_z + v_ys &\n        (1 - c) v_y v_z - v_xs &\n        (1 - c) v_z^2 + c &\n        0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}.\n\\end{align*}\n\nWhere c = \\cos(\\theta) and s = \\sin(\\theta). Again, you don’t really need to know how this is derived but if you are curious click on the dropdown link below.\n\nDerivation of the axis-angle rotation matrix (click to show)\n\nThe rotation about the unit vector \\hat{v} = (v_x, v_y, v_z) by angle \\theta is the \n\ncomposition of 5 separate rotations:\n\nRotate \\hat{v} around the x-axis so that it is in the xz-plane (the y component of the vector is 0);\n\nRotate the vector around the y-axis so that it points along the z-axis (the x and y components are 0 and the z component is a positive number);\n\nPerform the rotation around the z-axis;\n\nReverse the rotation around the y-axis;\n\nReverse the rotation around the x-axis.\n\nThe rotation around the x-axis is achieved by forming a right-angled triangle in the yz-plane where the the angle of rotation \\theta has an adjacent side of length v_z, an opposite side of length v_y and a hypotenuse of length \\sqrt{v_y^2 + v_z^2} (\n\nFigure 13).\n\n\n\nFigure 13:Rotate \\vec{v} around the x-axis\n\nTherefore \\cos(\\theta) = \\dfrac{v_z}{\\sqrt{v_y^2 + v_z^2}} and \\sin(\\theta) = \\dfrac{v_y}{\\sqrt{v_y^2 + v_z^2}} so the rotation matrix isR_1 =\n\\begin{pmatrix}\n    1 & 0 & 0 & 0 \\\\\n    0 & \\dfrac{v_z}{\\sqrt{v_y^2 + v_z^2}} & -\\dfrac{v_x}{\\sqrt{v_y^2 + v_z^2}} & 0 \\\\\n    0 & \\dfrac{v_y}{\\sqrt{v_y^2 + v_z^2}} &  \\dfrac{v_z}{\\sqrt{v_y^2 + v_z^2}} & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nThe rotation around the y-axis is achieved by forming another right-angled triangle in the xz-plane where \\theta has an adjacent side of length \\sqrt{v_y^2 + v_z^2}, an opposite side of length v_x and a hypotenuse of length 1 since \\hat{v} is a unit vector (\n\nFigure 14).\n\n\n\nFigure 14:Rotate around the y-axis\n\nTherefore, \\cos(\\theta) = \\sqrt{v_y^2 + v_z^2} and \\sin(\\theta) = v_x. Note that here we are rotating in the clockwise direction so the rotation matrix isR_2 = \\begin{pmatrix}\n    \\sqrt{v_y^2 + v_z^2} & 0 & -v_x & 0 \\\\\n    0 & 1 & 0 & 0 \\\\\n    v_x & 0 & \\sqrt{v_y^2 + v_z^2} & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nNow that the vector points along the z-axis we perform the rotation so the rotation matrix for this isR_3 = \\begin{pmatrix}\n    \\cos(\\theta) & -\\sin(\\theta) & 0 & 0 \\\\\n    \\sin(\\theta) &  \\cos(\\theta) & 0 & 0 \\\\\n    0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 1\n\\end{pmatrix}.\n\nThe reverse rotation around the y is simply the rotation matrix R_2 with the negative sign for \\sin(\\theta) swapped\\begin{align*}\n    R_4 &= \\begin{pmatrix}\n       \\sqrt{v_y^2 + v_z^2} & 0 & v_x & 0 \\\\\n       0 & 1 & 0 & 0 \\\\\n       -v_x & 0 & \\sqrt{v_y^2 + v_z^2} & 0 \\\\\n       0 & 0 & 0 & 1\n   \\end{pmatrix}.\n\\end{align*}\n\nThe reverse rotation around the x is simply the rotation matrix R_1 with the negative sign for \\sin(\\theta) swapped\\begin{align*}\n   R_5 &= \\begin{pmatrix}\n       1 & 0 & 0 & 0 \\\\\n       0 &  \\dfrac{v_z}{\\sqrt{v_y^2 + v_z^2}} & \\dfrac{v_y}{\\sqrt{v_y^2 + v_z^2}} & 0 \\\\\n       0 & -\\dfrac{v_x}{\\sqrt{v_y^2 + v_z^2}} & \\dfrac{v_z}{\\sqrt{v_y^2 + v_z^2}} & 0 \\\\\n       0 & 0 & 0 & 1\n   \\end{pmatrix}.\n\\end{align*}\n\nMultiplying all the separate matrices together gives\\begin{align*}\n    Rotate &= R_5 \\cdot R_4 \\cdot R_3 \\cdot R_2 \\cdot R_1 \\\\\n    &=\n    \\begin{pmatrix}\n        \\dfrac{v_x^2 + (v_y^2 + v_z^2)c}{\\|\\vec{v}\\|^2} &\n        \\dfrac{v_xv_y(1 - c)}{\\|\\vec{v}\\|^2} + \\dfrac{v_zs}{\\|\\vec{v}\\|} &\n        \\dfrac{v_xv_z(1 - c)}{\\|\\vec{v}\\|^2} - \\dfrac{v_ys}{\\|\\vec{v}\\|} &\n        0 \\\\\n        \\dfrac{v_xv_y(1 - c)}{\\|\\vec{v}\\|^2} - \\dfrac{v_zs}{\\|\\vec{v}\\|} &\n        \\dfrac{v_y^2 + (v_x^2 + v_y^2)c}{\\|\\vec{v}\\|^2} &\n        \\dfrac{v_yv_z(1 - c)}{\\|\\vec{v}\\|^2} - \\dfrac{v_xs}{\\|\\vec{v}\\|} &\n        0 \\\\\n        \\dfrac{v_xv_z(1 - c)}{\\|\\vec{v}\\|^2} + \\dfrac{v_ys}{\\|\\vec{v}\\|} &\n        \\dfrac{v_yv_z(1 - c)}{\\|\\vec{v}\\|^2} - \\dfrac{v_xs}{\\|\\vec{v}\\|} &\n        \\dfrac{v_z^2 + (v_x^2 + v_y^2)c}{\\|\\vec{v}\\|^2} &\n        0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}.\n\\end{align*}\n\nWhere c = \\cos(\\theta) and s = \\sin(\\theta). Substituting v_y^2 + v_z^2 = 1 - v_x^2 and the matrix simplifies to\\begin{align*}\n    Rotate &= R_1 \\cdot R_2 \\cdot R_3 \\cdot R_4 \\cdot R_5 \\\\\n    &=\n    \\begin{pmatrix}\n        (1 - c) v_x^2 + c &\n        (1 - c) v_x v_y - v_zs &\n        (1 - c) v_x v_z + v_ys & 0 \\\\\n        (1 - c) v_x v_y + v_zs &\n        (1 - c) v_y^2 + c &\n        (1 - c) v_y v_z - v_xs & 0 \\\\\n        (1 - c) v_x v_z - v_ys &\n        (1 - c) v_y v_z + v_xs &\n        (1 - c) v_z^2 + c & 0 \\\\\n        0 & 0 & 0 & 1\n    \\end{pmatrix}.\n\\end{align*}\n\nNote that this matrix is transposed when we code it into JavaScript.\n\nThe rotations around the three coordinates axis can be calculated using the axis-angle rotation matrix (by letting \\hat{v} be (1,0,0), (0,1,0) or (0,0,1) for rotating around the x, y and z axes respectively) so we can edit our rotate() function so that it uses equation \n\n(24).\n\nTask\n\nEdit the rotate() function in the maths.js file so that it looks like the following.rotate(x, y, z, rad) {\n const len = Math.sqrt(x * x + y * y + z * z);\n if (len > 0) {\n   x /= len; y /= len; z /= len;\n }\n const c = Math.cos(rad);\n const s = Math.sin(rad);\n const t = 1 - c;\n\n return new Mat4().set(\n   t * x * x + c,      t * x * y + s * z,  t * x * z - s * y,  0,\n   t * y * x - s * z,  t * y * y + c,      t * y * z + s * x,  0,\n   t * z * x + s * y,  t * z * y - s * x,  t * z * z + c,      0,\n   0, 0, 0, 1\n );\n}\n\nAnd change the calculation of the rotation matrix to the followingconst rotate    = new Mat4().rotate(0, 0, 1, angle);\n\nHere we have changed our function for calculating the rotation matrix so that it uses axis-angle rotation and have used it to rotate the rectangle by 45^\\circ anti-clockwise about a vector pointing along the z-axis (i.e., straight out of the screen towards you). Refreshing your browser and you should see that the output doesn’t change (\n\nFigure 11).","type":"content","url":"/transformations#axis-angle-rotation-section","position":11},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Composite transformations"},"type":"lvl2","url":"/transformations#composite-transformations-section","position":12},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Composite transformations"},"content":"So far we have performed translation, scaling and rotation transformations on our rectangle separately. What if we wanted to combine these transformations so that we can control the size, rotation and position of the rectangle? If we apply the transformations in the order scale then rotate then translate then applying the scaling we have\\begin{align*}\n    \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ 1 \\end{pmatrix}\n    &=\n    Scale \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}.\n\\end{align*}\n\nNext applying rotation to the scaled coordinates we have\\begin{align*}\n    \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ 1 \\end{pmatrix} &=\n    Rotate \\cdot Scale \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}.\n\\end{align*}\n\nFinally applying translation to the scaled and rotated coordinates we have\\begin{align*}\n    \\begin{pmatrix} x' \\\\ y' \\\\ z' \\\\ 1 \\end{pmatrix}\n    &=\n    Translate \\cdot Rotate \\cdot Scale \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}.\n\\end{align*}\n\nTranslate \\cdot Rotate \\cdot Scale is a single 4 \\times 4 transformation matrix that combines the three transformations known as the model matrix. Note the order that the translations are applied to the coordinates is read from right to left.\n\nLets apply scaling, rotation and translation (in that order) to our rectangle. Since we have already calculated the separate transformation matrices all we need to do is to multiply them together when calculating the model matrix.\n\nTask\n\nChange the model matrix to the following.const model = translate.multiply(rotate).multiply(scale);\n\nRefresh your web browser and you should see that the rectangle has been scaled down, rotated anti-clockwise and then translated as shown in \n\nFigure 15.\n\n\n\nFigure 15:Scaling, rotation and translation applied to the textured rectangle.","type":"content","url":"/transformations#composite-transformations-section","position":13},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Animation"},"type":"lvl2","url":"/transformations#animating-objects-section","position":14},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Animation"},"content":"We are now going to introduce animation to our WebGL application so that we can better see the effects of animations. Animation is done by redrawing the scene while updating values that represent motion or change, such as the position and size of an object. In WebGL this is done using the brower’s built-in function\n\nrequestAnimationFrame(callback)\n\nThis schedules the rendering function to run before the next screen refresh (typically 60 times per second). The callback() function is used to update the animation state (e.g., move, scale, rotate objects), read input (e.g., from keyboad and mouse), set the shader uniforms (e.g., transformation matrices), and draw the frame. The callback recieves a timestamp that is the time in milliseconds since the rendering of the last frame which is useful for controlling movement speed.\n\nWe are going to make a few changes to our transformations.js file to animate the rectangle.\n\nTask\n\nFirst comment out (or delete) the code used to calculate the transformation matrices, the model matrix and sending the model matrix to the shader and also the draw commands (trust me).\n\nThen add the following function definite to the transformations.js file before the main() function.// Render frame\nfunction render(time) {\n\n  // Clear canvas\n  gl.clear(gl.COLOR_BUFFER_BIT);\n\n  // Calculate transformation matrices\n  const translate = new Mat4().translate(0.4, 0.3, 0);\n  const scale     = new Mat4().scale(0.5, 0.4, 1);\n  const angle     = 1/2 * time * 0.001 * 2 * Math.PI;\n  const rotate    = new Mat4().rotate(0, 0, 1, angle);\n\n  // Calculate transformation matrix and send it to the shader\n  const model = translate.multiply(rotate).multiply(scale);\n  gl.uniformMatrix4fv(gl.getUniformLocation(shaderProgram, \"uModel\"), false, model.m);\n\n  // Draw triangles\n  gl.bindVertexArray(VAO);\n  gl.drawElements(gl.TRIANGLES, 6, gl.UNSIGNED_SHORT, 0);\n\n  // Call the render function before the next screen refresh\n  requestAnimationFrame(render);\n}\n\nNew add a call to this callback function at the end of the main() function// Call the render function\nrender();\n\nHere we have defined our callback function for the requestAnimationFrame() function. Note that this contains the code that you have deleted from the main() function and includes a call to requestAnimationFrame(render) which will re-render the frame when the browser is ready.\n\nRefresh your web browser and you should see the same rectangle from \n\nFigure 15. Although this may seem unimpressive, what is happening here is instead of rendering a single static frame, the canvas is being refreshed 60 times per second.\n\nLet rotate our rectangle about its centre.\n\nTask\n\nChange the command used to calculate the rotation matrix to the following.const angle     = 1/2 * time * 0.001 * 2 * Math.PI;\nconst rotate    = new Mat4().rotate(0, 0, 1, angle);\n\nHere we calculate the rotation angle so that the rectangle will complete one full rotation every 2 seconds. Note that time is the time in milliseconds since the application was started hence we multiply the number of radians in a circle, 2\\pi, by 0.001.\n\nRefresh your browser and you should see something similar to below.\n\n\n\nWhen calculating the composite transformation matrix the order in which we multiply the individual transformations will determine the effects of the composite transformation. To see this lets translate the rectangle first before rotating it.\n\nTask\n\nChange the calculation of the model matrix so that it looks like the following.const model = rotate.multiply(translate).multiply(scale);\n\nHere we have changed the order which the transformation matrices for translation and rotation are switched, i.e.,Model = Rotate \\cdot Translate \\cdot Scale,\n\nwhich has the effect of moving the rectangle so that it is centred at coordinates (0.4, 0.3, 0) and then rotated about (0, 0, 0). Refresh your browser and you should see something similar to below.\n\n","type":"content","url":"/transformations#animating-objects-section","position":15},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Exercises"},"type":"lvl2","url":"/transformations#exercises","position":16},{"hierarchy":{"lvl1":"Lab 5: Transformations","lvl2":"Exercises"},"content":"Scale the original rectangle so that it is a quarter of the original size and apply translation so that the rectangle moves anti-clockwise around a circle centred at the window centre with radius 0.5 and completes one full rotation every 5 seconds. Hint: the coordinates of points on a circle centered at (0,0) with radius r can be calculated using x = r\\cos(t) and y = r\\sin(t) where t is some number.\n\n\n\nRotate your rectangle from exercise 1 in a clockwise rotation about its centre at twice the rotation speed used in exercise 1.\n\n\n\nScale your rectangle from exercise 2 so that it grows and shrinks about its centre. Hint: The \\sin(t) function oscillates between 0 and 1 as t increases.\n\n\n\nTranslate the rectangle so that it moves around the canvas and bounces off the borders.\n\n","type":"content","url":"/transformations#exercises","position":17},{"hierarchy":{"lvl1":"Computer Graphics"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Computer Graphics"},"content":"This site contains the notes used for the lab sessions for the Computer Graphics module. They are designed for students to work through in the labs with the help of the tutors. The content of each of the labs have been organised such that each one uses code from the preceding labs, so you will need to make sure you have completed each lab before moving onto the next one.\n\nWeek\n\nDate (w/c)\n\nContent\n\n1\n\n26/01/2026\n\nCore Computer Graphics Concepts: WebGL, the graphics pipeline, glossary of terms. Hello Triangle: vertex buffers, vertex and fragment shaders\n\n2\n\n02/02/2026\n\nVertex Colours, More Shapes & VAOs\n\n3\n\n09/02/2026\n\nTextures\n\n4\n\n16/02/2026\n\nVectors and Matrices\n\n5\n\n23/02/2026\n\nTransformations\n\n6\n\n02/03/2026\n\n3D Worlds\n\n7\n\n09/03/2026\n\nMoving the Camera\n\n8\n\n16/03/2026\n\nLighting Models\n\n9\n\n23/03/2026\n\nNormal Maps\n\n10\n\n20/04/2026\n\nQuaternions","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Computer Graphics","lvl2":"Core Computer Graphics Concepts"},"type":"lvl2","url":"/#core-computer-graphics-concepts","position":2},{"hierarchy":{"lvl1":"Computer Graphics","lvl2":"Core Computer Graphics Concepts"},"content":"","type":"content","url":"/#core-computer-graphics-concepts","position":3},{"hierarchy":{"lvl1":"Computer Graphics","lvl3":"The Graphics Pipeline","lvl2":"Core Computer Graphics Concepts"},"type":"lvl3","url":"/#the-graphics-pipeline","position":4},{"hierarchy":{"lvl1":"Computer Graphics","lvl3":"The Graphics Pipeline","lvl2":"Core Computer Graphics Concepts"},"content":"The graphics pipeline (or rendering pipeline) is a sequence of steps the GPU takes to convert 3D scene data (e.g., vertices, textures, lighting information, etc.) into a 2D image displayed on the screen. Like many graphics APIs, WebGL uses vertex shaders and fragment shaders, small programs that are written in GLSL (OpenGL Shading Language) which are run directly on the GPU to perform these steps.\n\nThe stages of the graphics pipeline are:\n\nVertex specification -- the geometry of objects that construct a 3D scene are defined using arrays of vertices. Each vertex is a collection of data, typically the co-ordinates of the vertex position but can also include texture co-ordinates, surface normal vectors and other attributes (more on these later). In WebGL we create Vertex Buffer Objects (VBO) to store this data and tell WebGL how to interpret them.\n\nVertex shader -- each vertex is processed by a vertex shader whose job it is to transform the 3D co-ordinates from the model space (the local object co-ordinates) into clip space (the co-ordinate system the GPU uses to determine what is visible on screen). The vertex shader is called once per vertex.\n\nClipping -- the vertex outputs from the vertex shader are grouped into primitives (usually triangles) and clipped to the clip space such that any primitive that lie outside the clip space are ignored. Primitives that lie partially outside the clip space are cut so that the part that is within the clip space is retained.\n\nRasterisation -- the primitives are converted into grids of pixels known as fragments. The interior fragments of a primitive are “filled in” by interpolating the vertex data across the surface.\n\nFragment shader -- each fragment is processed by the fragment shader which computes the final colour. This can be based on vertex data, texture mapping, lighting models and other visual effects.\n\nPer-fragment operations -- before a fragment is displayed (i.e., becomes a pixel on the screen), depth testing (is it hidden by something else), alpha testing (for transparency) and stencil testing (for masking). Fragments can be discarded at this stage.\n\nFrame buffer -- after processing the fragment is written to the frame buffer which is then is sent to the display.","type":"content","url":"/#the-graphics-pipeline","position":5}]}